{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8668186,"sourceType":"datasetVersion","datasetId":5106341}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch_geometric","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-12T01:34:03.035775Z","iopub.execute_input":"2024-06-12T01:34:03.036115Z","iopub.status.idle":"2024-06-12T01:34:17.545180Z","shell.execute_reply.started":"2024-06-12T01:34:03.036084Z","shell.execute_reply":"2024-06-12T01:34:17.544136Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install tensorboard","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:34:17.547527Z","iopub.execute_input":"2024-06-12T01:34:17.547939Z","iopub.status.idle":"2024-06-12T01:34:29.685865Z","shell.execute_reply.started":"2024-06-12T01:34:17.547901Z","shell.execute_reply":"2024-06-12T01:34:29.684797Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport imageio.v2 as imageio\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom skimage import io, segmentation\nfrom torchvision import transforms\nimport torch.nn as nn\nimport numpy as np\nimport tifffile as tiff\nfrom skimage.segmentation import slic, felzenszwalb, quickshift\nfrom skimage.segmentation import mark_boundaries\nfrom torchvision.transforms import functional as F\n# import mahotas as mh\nfrom skimage.measure import regionprops,label as label_image_regions\n\nfrom torch import optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom timeit import default_timer as timer\nimport time\n\nimport networkx as nx\nfrom torch_geometric.utils import grid, subgraph,to_networkx,select\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import ConcatDataset, random_split\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, PrecisionRecallDisplay\nimport seaborn as sns\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:34:29.687713Z","iopub.execute_input":"2024-06-12T01:34:29.688585Z","iopub.status.idle":"2024-06-12T01:34:48.051357Z","shell.execute_reply.started":"2024-06-12T01:34:29.688525Z","shell.execute_reply":"2024-06-12T01:34:48.050393Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from torch_geometric.data import Dataset, Data\n# from torch_geometric.utils import grid, subgraph, select\n# import os\n# import imageio\n# import torch\n# import os.path as osp\n\n# class PotsDamGraphDataset(Dataset):\n#     def __init__(self, root_dir,out_dir, mode='train', transform=None):\n#         self.root_dir = root_dir\n#         self.mode = mode\n#         self.transform = transform\n#         self.images_folder = os.path.join(self.root_dir, f'images_{mode}')\n#         self.meta_dir = os.path.join(self.root_dir, f'metadata_{mode}')\n#         self.mask_dir = os.path.join(self.root_dir, f'masks_{mode}')\n\n#         self.image_files = os.listdir(self.images_folder)\n#         self.image_files.sort()\n\n#         # Create directory for saving data based on mode\n#         self.proc_dir = os.path.join(out_dir, f'processed_{mode}')\n#         if not os.path.exists(self.proc_dir):\n#             os.makedirs(self.proc_dir)\n\n#     def __len__(self):\n#         return len(self.image_files)\n\n#     def __getitem__(self, idx):\n#         img_name = self.image_files[idx]\n#         img_path = os.path.join(self.images_folder, img_name)\n#         image = imageio.v2.imread(img_path)\n\n#         # Extract index and segment index from the image name\n#         sam_idx, _, _, segment_idx = img_name.split('_')\n#         sam_idx = int(sam_idx)\n#         segment_idx = int(segment_idx.split('.')[0])  # Remove '.tif' extension\n\n#         # Load metadata\n#         meta_filename = f\"{sam_idx}_{self.mode}_metadata_{segment_idx}.txt\"\n#         meta_path = os.path.join(self.meta_dir, meta_filename)\n#         with open(meta_path, 'r') as f:\n#             metadata_lines = f.readlines()\n\n#         # Extract label from the first line (index 1)\n#         label = int(metadata_lines[0].split(':')[1])\n\n#         # Extract centroid coordinates from the second line (index 1)\n#         centroid_line = metadata_lines[1]\n#         _, centroid_coords_str = centroid_line.split(': ')\n#         centroid_coords_str = centroid_coords_str.strip()\n#         centroid_coords_str = centroid_coords_str.replace('(', '').replace(')', '')\n#         centroid_x_str, centroid_y_str = centroid_coords_str.split(', ')\n#         centroid_coords = (float(centroid_x_str), float(centroid_y_str))\n#         #centroid_coords = tuple(map(float, centroid_coords_str.split(',')))\n\n#         # Load mask\n#         mask_filename = f\"{sam_idx}_{self.mode}_mask_{segment_idx}.tif\"\n#         mask_path = os.path.join(self.mask_dir, mask_filename)\n#         mask = imageio.v2.imread(mask_path)\n#         mask_tensor = torch.tensor(mask == True, dtype=torch.bool)\n\n#         # Normalize image\n#         normalized_image = image / 255.0\n#         image_tensor = torch.tensor(normalized_image, dtype=torch.float)\n#         centroid_tensor = torch.tensor(centroid_coords, dtype=torch.float)\n\n#         # Compute edge indices and positions\n#         height = image_tensor.shape[0]\n#         width = image_tensor.shape[1]\n\n#         image_grid = grid(height, width, dtype=torch.long)\n#         edge_index, pos = image_grid\n\n#         # Generate subgraphs\n#         mask_idx = mask_tensor.flatten()\n#         sub_graph_edge_index, _ = subgraph(mask_idx, edge_index, relabel_nodes=True)\n\n#         # Get node features and positions corresponding to nodes in the subgraph\n#         img = image_tensor.reshape(height * width, 4)\n#         subgraph_img = select(img, mask_idx, dim=0)\n#         subgraph_pos = select(pos, mask_idx, dim=0)\n\n#         # Create Data object\n#         data = Data(x=subgraph_img, edge_index=sub_graph_edge_index, y=label, pos=subgraph_pos)\n\n#         # Save data to disk\n#         data_path = osp.join(self.processed_dir, f'data_{idx}.pt')\n#         torch.save(data, data_path)\n\n#         return data\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:34:48.053748Z","iopub.execute_input":"2024-06-12T01:34:48.054325Z","iopub.status.idle":"2024-06-12T01:34:48.061431Z","shell.execute_reply.started":"2024-06-12T01:34:48.054298Z","shell.execute_reply":"2024-06-12T01:34:48.060589Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# root_dir = '/kaggle/input/deepfeats-2/DeepFeats_Update2/' \n# out_dir= '/kaggle/working/'# Update this with the actual path to your data\n# train_dataset = PotsDamGraphDataset(root_dir,out_dir, mode='train')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:34:48.062539Z","iopub.execute_input":"2024-06-12T01:34:48.062942Z","iopub.status.idle":"2024-06-12T01:34:48.089673Z","shell.execute_reply.started":"2024-06-12T01:34:48.062915Z","shell.execute_reply":"2024-06-12T01:34:48.088850Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# val_dataset = PotsDamGraphDataset(root_dir,out_dir, mode='val')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:34:48.090721Z","iopub.execute_input":"2024-06-12T01:34:48.091018Z","iopub.status.idle":"2024-06-12T01:34:48.099325Z","shell.execute_reply.started":"2024-06-12T01:34:48.090994Z","shell.execute_reply":"2024-06-12T01:34:48.098616Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_dataset = PotsDamGraphDataset(root_dir,out_dir, mode='test')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:34:48.100525Z","iopub.execute_input":"2024-06-12T01:34:48.100944Z","iopub.status.idle":"2024-06-12T01:34:48.109868Z","shell.execute_reply.started":"2024-06-12T01:34:48.100913Z","shell.execute_reply":"2024-06-12T01:34:48.109001Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PotsDamGraphDataset(Dataset):\n    def __init__(self, root_dir, mode='train', transform=None):\n        self.root_dir = root_dir\n        self.mode = mode\n        self.transform = transform\n        self.images_folder = os.path.join(self.root_dir, f'images_{mode}')\n        self.meta_dir = os.path.join(self.root_dir, f'metadata_{mode}')\n        self.mask_dir = os.path.join(self.root_dir, f'masks_{mode}')\n\n        self.image_files = os.listdir(self.images_folder)\n        self.image_files.sort()\n\n\n\n\n    def __len__(self):\n        return len(self.image_files)\n\n\n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n        img_path = os.path.join(self.images_folder, img_name)\n        image = imageio.imread(img_path)\n\n        # Extract index and segment index from the image name\n        sam_idx, _, _, segment_idx = img_name.split('_')\n        sam_idx = int(sam_idx)\n        segment_idx = int(segment_idx.split('.')[0])  # Remove '.tif' extension\n\n        # Load metadata\n        meta_filename = f\"{sam_idx}_{self.mode}_metadata_{segment_idx}.txt\"\n        meta_path = os.path.join(self.meta_dir, meta_filename)\n        with open(meta_path, 'r') as f:\n            metadata_lines = f.readlines()\n\n        # Extract label from the first line (index 1)\n        label = int(metadata_lines[0].split(':')[1])\n#         centroid = tuple(map(float, metadata[1].split(':')[1].split(',')))\n\n        # Extract centroid coordinates from the second line (index 1)\n        centroid_line = metadata_lines[1]\n\n        # Split the line by colon and whitespace to isolate coordinate values\n        _, centroid_coords_str = centroid_line.split(': ')\n        centroid_coords_str = centroid_coords_str.strip()  # Remove leading/trailing whitespace\n\n        # Remove parentheses from coordinate string and split by comma to separate x and y\n        centroid_coords_str = centroid_coords_str.replace('(', '').replace(')', '')\n        centroid_x_str, centroid_y_str = centroid_coords_str.split(', ')\n\n        # Convert coordinate strings to floats and create a tuple\n        centroid = (float(centroid_x_str), float(centroid_y_str))\n        label_tensor = torch.tensor(label, dtype=torch.long)\n\n        # Load mask\n        mask_filename = f\"{sam_idx}_{self.mode}_mask_{segment_idx}.tif\"\n        mask_path = os.path.join(self.mask_dir, mask_filename)\n        mask = imageio.imread(mask_path)\n        mask_tensor = torch.tensor(mask, dtype=torch.bool)\n\n        # Normalize image\n        normalized_image = image / 255.0\n        image_tensor = torch.tensor(normalized_image, dtype=torch.float)\n        centroid_tensor = torch.tensor(centroid, dtype=torch.float)\n\n\n        sample = {'image': image_tensor, 'label': label_tensor,'mask': mask_tensor,'centroid':centroid_tensor}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:34:48.111243Z","iopub.execute_input":"2024-06-12T01:34:48.111630Z","iopub.status.idle":"2024-06-12T01:34:48.126892Z","shell.execute_reply.started":"2024-06-12T01:34:48.111600Z","shell.execute_reply":"2024-06-12T01:34:48.125933Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RegionToGraph(nn.Module):\n    def __init__(self):\n        super(RegionToGraph, self).__init__()\n\n    def forward(self,sample):\n        image = sample['image']\n        label = sample['label']\n        mask = sample['mask']\n        centroid = sample['centroid']\n        #compute edge indices and positions\n        height = image.shape[0]\n        width = image.shape[1]\n\n        image_grid = grid(height, width, dtype=torch.long)\n        edge_index, pos = image_grid\n\n        #generate subgraphs\n        mask_idx=(mask == True).flatten()\n        sub_graph_edge_index, _ = subgraph(mask_idx,edge_index,relabel_nodes=True)\n\n        # Get node features and positions corresponding to nodes in the subgraph\n        img=image.reshape(height*width,4)\n        subgraph_img = select(img,mask_idx,dim=0)\n        subgraph_pos = select(pos,mask_idx,dim=0)\n\n        #compute pos relative to centroid\n        #pos_rel = subgraph_pos-centroid\n\n        # Concatenate 'subgraph_img' and 'pos_rel' along the feature dimension\n        #node_feats = torch.cat([subgraph_img, pos_rel], dim=1)\n\n        # Create Data object to represent the subgraph\n        subgraph_data = Data(x=subgraph_img, edge_index=sub_graph_edge_index, y=label, pos=subgraph_pos)\n\n        return subgraph_data\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:34:48.128037Z","iopub.execute_input":"2024-06-12T01:34:48.128631Z","iopub.status.idle":"2024-06-12T01:34:48.141529Z","shell.execute_reply.started":"2024-06-12T01:34:48.128600Z","shell.execute_reply":"2024-06-12T01:34:48.140750Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import torch\n\n# class SaveGraphTransform(object):\n#     def __init__(self, save_dir):\n#         self.save_dir = save_dir\n\n#         if not os.path.exists(self.save_dir):\n#             os.makedirs(self.save_dir)\n\n#     def __call__(self, subgraph_data):\n\n#         # Define the file path to save the graph data\n#         graph_path = os.path.join(self.save_dir, f'graph_{torch.randint(1000000, ())}.pt')\n\n#         # Save the graph data\n#         torch.save(subgraph_data, graph_path)\n\n#         # Return the original sample\n#         return sample\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:34:48.144222Z","iopub.execute_input":"2024-06-12T01:34:48.144705Z","iopub.status.idle":"2024-06-12T01:34:48.156240Z","shell.execute_reply.started":"2024-06-12T01:34:48.144679Z","shell.execute_reply":"2024-06-12T01:34:48.155480Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = '/kaggle/input/deepfeats-2/DeepFeats_Update2/'\n\n# Create the transform\ntransform = RegionToGraph()\n\nmode='train'\ntrain_dataset = PotsDamGraphDataset(root_dir, mode=mode,transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:36:04.050513Z","iopub.execute_input":"2024-06-12T01:36:04.051505Z","iopub.status.idle":"2024-06-12T01:36:04.121333Z","shell.execute_reply.started":"2024-06-12T01:36:04.051468Z","shell.execute_reply":"2024-06-12T01:36:04.120138Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mode='test'\ntest_dataset = PotsDamGraphDataset(root_dir, mode=mode,transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:36:18.933579Z","iopub.execute_input":"2024-06-12T01:36:18.934437Z","iopub.status.idle":"2024-06-12T01:36:19.005410Z","shell.execute_reply.started":"2024-06-12T01:36:18.934393Z","shell.execute_reply":"2024-06-12T01:36:19.004160Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mode='val'\nval_dataset = PotsDamGraphDataset(root_dir, mode=mode,transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:31:24.082256Z","iopub.status.idle":"2024-06-12T01:31:24.082662Z","shell.execute_reply.started":"2024-06-12T01:31:24.082463Z","shell.execute_reply":"2024-06-12T01:31:24.082478Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Concatenate the train and validation datasets\ncombined_dataset = ConcatDataset([train_dataset, val_dataset])\n# Load targets from the saved numpy file\ntargets = np.load('/kaggle/input/deepfeats-2/targets (2).npy')\n\n# Define the sizes for the new splits\nVAL_SIZE = 0.1\nSEED = 42\n\n# Perform the new split\ntrain_indices, val_indices = train_test_split(\n    range(len(combined_dataset)),\n    test_size=VAL_SIZE,\n    random_state=SEED,\n    stratify=targets\n)\n\n# Generate subsets based on indices\nnew_train_dataset = Subset(combined_dataset, train_indices)\nnew_val_dataset = Subset(combined_dataset, val_indices)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:31:24.084234Z","iopub.status.idle":"2024-06-12T01:31:24.084684Z","shell.execute_reply.started":"2024-06-12T01:31:24.084456Z","shell.execute_reply":"2024-06-12T01:31:24.084495Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameters and device setup\nLR = 0.001\nepochs = 20\nbatch_size = 10\nnum_features = 4\nnum_classes = 7\nout_channels = 64\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nexperiment = f'epochs_{epochs}_batch_size_{batch_size}_CNN_lr_{LR}'\nlog_dir = f'_{experiment}'\nwriter = SummaryWriter(comment=log_dir)\n\n# train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=False)\n# test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=False)\n\n#or\n\ntrain_loader = DataLoader(new_train_dataset, batch_size=batch_size,num_workers=4,pin_memory=True, shuffle=True,persistent_workers=True)\nval_loader = DataLoader(new_val_dataset, batch_size=batch_size,num_workers=4,pin_memory=True, shuffle=False,persistent_workers=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size,num_workers=4,pin_memory=True, shuffle=False,persistent_workers=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:31:24.085988Z","iopub.status.idle":"2024-06-12T01:31:24.086339Z","shell.execute_reply.started":"2024-06-12T01:31:24.086161Z","shell.execute_reply":"2024-06-12T01:31:24.086175Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torch.nn.functional as F\n# from torch.nn import Linear, BatchNorm1d\n# from torch_geometric.nn import GraphConv, global_max_pool as gmp\n\n# class GraphCon(torch.nn.Module):\n#     def __init__(self, num_features, hidden_channels, num_classes):\n#         super(GraphCon, self).__init__()\n#         torch.manual_seed(12345)\n        \n#         self.conv1 = GraphConv(num_features, hidden_channels)\n#         self.bn1 = BatchNorm1d(hidden_channels)\n        \n#         self.conv2 = GraphConv(hidden_channels, hidden_channels)\n#         self.bn2 = BatchNorm1d(hidden_channels)\n        \n#         self.conv3 = GraphConv(hidden_channels, hidden_channels)\n#         self.bn3 = BatchNorm1d(hidden_channels)\n        \n#         self.lin = Linear(hidden_channels, num_classes)\n\n#     def forward(self, x, edge_index, batch):\n#         x = self.conv1(x, edge_index)\n#         x = self.bn1(x)\n#         x = x.relu()\n        \n#         x = self.conv2(x, edge_index)\n#         x = self.bn2(x)\n#         x = x.relu()\n        \n#         x = self.conv3(x, edge_index)\n#         x = self.bn3(x)\n\n#         x = gmp(x, batch)  # Global max pooling\n\n#         x = F.dropout(x, p=0.5, training=self.training)\n#         x = self.lin(x)\n        \n#         return x\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:31:24.088169Z","iopub.status.idle":"2024-06-12T01:31:24.088565Z","shell.execute_reply.started":"2024-06-12T01:31:24.088343Z","shell.execute_reply":"2024-06-12T01:31:24.088381Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.nn import Linear\nfrom torch_geometric.nn import GCNConv, global_mean_pool as gap, global_max_pool as gmp\nfrom torch_geometric.nn import MessagePassing\n\nclass GCN(MessagePassing):\n    def __init__(self, num_features, out_channels, num_classes):\n        super(GCN, self).__init__()\n        self.gcn1 = GCNConv(num_features, out_channels)\n        self.gcn2 = GCNConv(out_channels, out_channels)\n        self.gcn3 = GCNConv(out_channels, out_channels)\n        \n       \n        # So the out_channels * 2 for the concatenated pooled features\n        self.out = Linear(out_channels * 2, num_classes)\n\n    def forward(self, x, edge_index, batch_index):\n        x = self.gcn1(x, edge_index).relu()\n        x = self.gcn2(x, edge_index).relu()\n        x = self.gcn3(x, edge_index).relu()\n        \n        # Global Pooling (stack different aggregations)\n        x = torch.cat([gmp(x, batch_index), gap(x, batch_index)], dim=1)\n        \n        out = self.out(x)\n        return x,out","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:31:24.089702Z","iopub.status.idle":"2024-06-12T01:31:24.090076Z","shell.execute_reply.started":"2024-06-12T01:31:24.089892Z","shell.execute_reply":"2024-06-12T01:31:24.089908Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = GCN(num_features=num_features, num_classes=num_classes, out_channels=out_channels).to(device)\nprint(model)\n# model = model = GraphCon(num_features,hidden_channels=out_channels, num_classes=num_classes).to(device)\n# print(model)\nprint(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LR)\nscheduler = ReduceLROnPlateau(optimizer, mode='min')","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:37:54.947680Z","iopub.execute_input":"2024-06-11T22:37:54.948062Z","iopub.status.idle":"2024-06-11T22:37:55.221459Z","shell.execute_reply.started":"2024-06-11T22:37:54.948024Z","shell.execute_reply":"2024-06-11T22:37:55.220475Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# # Wrap your main training loop in a function to ensure proper cleanup\n# def train_model():\n#     best_val_loss = float('inf')\n#     patience = 5\n#     no_improvement_counter = 0\n#     all_true_labels = []\n#     all_predicted_labels = []\n\n#     try:\n#         for epoch in range(epochs):\n#             print(f'EPOCH {epoch + 1}:')\n#             start_time = timer()\n#             train_loss = 0.0\n#             correct_predictions = 0\n#             total_samples = 0\n\n#             model.train()\n#             for data in tqdm(train_loader):\n#                 data = data.to(device)\n\n#                 # Forward pass\n#                 y_hat = model(data.x, data.edge_index, data.batch)\n\n#                 # Compute the loss\n#                 loss = criterion(y_hat, data.y)\n#                 train_loss += loss.item()\n\n#                 # Backward pass and optimization step\n#                 optimizer.zero_grad()\n#                 loss.backward()\n#                 optimizer.step()\n\n#                 # Count correct predictions\n#                 _, predicted = torch.max(y_hat, 1)\n#                 correct_predictions += (predicted == data.y).sum().item()\n#                 total_samples += data.y.size(0)\n\n#             # Calculate accuracy and average loss for training\n#             train_accuracy = correct_predictions / total_samples\n#             train_average_loss = train_loss / total_samples\n\n#             # Log training loss and accuracy to TensorBoard\n#             writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch + 1)\n#             writer.add_scalar('loss/train', train_average_loss, epoch + 1)\n#             writer.add_scalar('acc/train', train_accuracy, epoch + 1)\n\n#             # Validation\n#             model.eval()\n#             valid_loss = 0.0\n#             correct_predictions_val = 0\n#             total_samples_val = 0\n\n#             with torch.no_grad():\n#                 for data_val in tqdm(val_loader):\n#                     data_val = data_val.to(device)\n\n#                     y_val_hat = model(data_val.x, data_val.edge_index, data_val.batch)\n\n#                     val_loss = criterion(y_val_hat, data_val.y)\n#                     valid_loss += val_loss.item()\n\n#                     _, predicted_val = torch.max(y_val_hat, 1)\n#                     correct_predictions_val += (predicted_val == data_val.y).sum().item()\n#                     total_samples_val += data_val.y.size(0)\n\n#                 scheduler.step(valid_loss)\n\n#                 # Calculate accuracy and average loss for validation\n#                 val_accuracy = correct_predictions_val / total_samples_val\n#                 val_average_loss = valid_loss / total_samples_val\n\n#                 # Log validation loss and accuracy to TensorBoard\n#                 writer.add_scalar('loss/val', val_average_loss, epoch + 1)\n#                 writer.add_scalar('acc/val', val_accuracy, epoch + 1)\n\n#             # Reset lists to collect true labels and predictions for the test dataset\n#             all_true_labels = []\n#             all_predicted_labels = []\n\n#             cum_test_loss = 0.0\n#             correct_predictions_test = 0\n#             total_samples_test = 0\n\n#             with torch.no_grad():\n#                 for data_test in tqdm(test_loader):\n#                     data_test = data_test.to(device)\n\n#                     y_test_hat = model(data_test.x, data_test.edge_index, data_test.batch)\n\n#                     test_loss = criterion(y_test_hat, data_test.y)\n#                     cum_test_loss += test_loss.item()\n\n#                     _, predicted_test = torch.max(y_test_hat, 1)\n#                     correct_predictions_test += (predicted_test == data_test.y).sum().item()\n#                     total_samples_test += data_test.y.size(0)\n\n#                     # Collect all true labels and predictions\n#                     all_true_labels.extend(data_test.y.cpu().numpy())\n#                     all_predicted_labels.extend(predicted_test.cpu().numpy())\n\n#                 # Calculate accuracy and average loss for testing\n#                 test_accuracy = correct_predictions_test / total_samples_test\n#                 test_average_loss = cum_test_loss / total_samples_test\n\n#                 # Log test loss and accuracy to TensorBoard\n#                 writer.add_scalar('loss/test', test_average_loss, epoch + 1)\n#                 writer.add_scalar('acc/test', test_accuracy, epoch + 1)\n\n#             print(f'LOSS train {train_average_loss}, valid {val_average_loss}, test {test_average_loss}')\n#             print(f'ACCURACY train {train_accuracy}, valid {val_accuracy}, test {test_accuracy}')\n\n#             # Early stopping\n#             if val_average_loss < best_val_loss:\n#                 best_val_loss = val_average_loss\n#                 no_improvement_counter = 0\n#                 # Save the current best model\n#                 best_model_path = f'./GAT_model_lr_{LR}.pth'\n#                 torch.save({'epoch': epoch,\n#                             'model_state': model.state_dict(),\n#                             'optimizer_state_dict': optimizer.state_dict()}, best_model_path)\n#             else:\n#                 no_improvement_counter += 1\n\n#             if patience is not None and no_improvement_counter >= patience:\n#                 print(f\"No improvement for {patience} epochs. Early stopping...\")\n#                 break\n\n#             end_time = timer()\n#             print(f\"Epoch {epoch + 1} completed. Total epoch time: {end_time - start_time:.3f} seconds\")\n\n#     except Exception as e:\n#         print(f\"An error occurred: {e}\")\n\n#     finally:\n#         # Generate classification report and confusion matrix\n#         all_true_labels = np.array(all_true_labels)\n#         all_predicted_labels = np.array(all_predicted_labels)\n#         class_names = ['Imp_Surf', 'Bldgs', 'Low_Veg', 'Tree', 'Car', 'Clutter']\n\n#         # Generate classification report\n#         class_report = classification_report(all_true_labels, all_predicted_labels, target_names=class_names, zero_division=0)\n#         print(\"Classification Report:\\n\", class_report)\n\n#         # Generate confusion matrix\n#         conf_matrix = confusion_matrix(all_true_labels, all_predicted_labels)\n\n#         # Plot confusion matrix\n#         fig, ax = plt.subplots(figsize=(10, 8))\n#         sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n#         ax.set_xlabel('Predicted Labels')\n#         ax.set_ylabel('True Labels')\n#         ax.set_title('Confusion Matrix')\n#         plt.show()\n\n#         # Optionally log classification report and confusion matrix to TensorBoard\n#         writer.add_text('Classification Report', class_report)\n#         fig_conf_matrix = plt.figure(figsize=(10, 8))\n#         sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n#         plt.xlabel('Predicted Labels')\n#         plt.ylabel('True Labels')\n#         plt.title('Confusion Matrix')\n#         writer.add_figure('Confusion Matrix', fig_conf_matrix)\n#         plt.close(fig_conf_matrix)\n\n# # Call the train_model function\n# train_model()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:37:55.223009Z","iopub.execute_input":"2024-06-11T22:37:55.223399Z","iopub.status.idle":"2024-06-11T22:37:55.236061Z","shell.execute_reply.started":"2024-06-11T22:37:55.223373Z","shell.execute_reply":"2024-06-11T22:37:55.235176Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.tensorboard import SummaryWriter\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom timeit import default_timer as timer\nfrom tqdm import tqdm\nimport numpy as np\n\n# Define the train_model function\ndef train_model():\n    best_val_loss = float('inf')\n    patience = 5\n    no_improvement_counter = 0\n\n    try:\n        for epoch in range(epochs):\n            print(f'EPOCH {epoch + 1}:')\n            start_time = timer()\n            train_loss = 0.0\n            correct_predictions = 0\n            total_samples = 0\n\n            model.train()\n            for data in tqdm(train_loader):\n                data = data.to(device)\n\n                # Forward pass\n                y_hat = model(data.x, data.edge_index, data.batch)\n\n                # Compute the loss\n                loss = criterion(y_hat, data.y)\n                train_loss += loss.item()\n\n                # Backward pass and optimization step\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                # Count correct predictions\n                _, predicted = torch.max(y_hat, 1)\n                correct_predictions += (predicted == data.y).sum().item()\n                total_samples += data.y.size(0)\n\n            # Calculate accuracy and average loss for training\n            train_accuracy = correct_predictions / total_samples\n            train_average_loss = train_loss / total_samples\n\n            # Log training loss and accuracy to TensorBoard\n            writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch + 1)\n            writer.add_scalar('loss/train', train_average_loss, epoch + 1)\n            writer.add_scalar('acc/train', train_accuracy, epoch + 1)\n\n            # Validation\n            model.eval()\n            valid_loss = 0.0\n            correct_predictions_val = 0\n            total_samples_val = 0\n\n            with torch.no_grad():\n                for data_val in tqdm(val_loader):\n                    data_val = data_val.to(device)\n\n                    y_val_hat = model(data_val.x, data_val.edge_index, data_val.batch)\n\n                    val_loss = criterion(y_val_hat, data_val.y)\n                    valid_loss += val_loss.item()\n\n                    _, predicted_val = torch.max(y_val_hat, 1)\n                    correct_predictions_val += (predicted_val == data_val.y).sum().item()\n                    total_samples_val += data_val.y.size(0)\n\n                scheduler.step(valid_loss)\n\n                # Calculate accuracy and average loss for validation\n                val_accuracy = correct_predictions_val / total_samples_val\n                val_average_loss = valid_loss / total_samples_val\n\n                # Log validation loss and accuracy to TensorBoard\n                writer.add_scalar('loss/val', val_average_loss, epoch + 1)\n                writer.add_scalar('acc/val', val_accuracy, epoch + 1)\n\n            print(f'LOSS train {train_average_loss}, valid {val_average_loss}')\n            print(f'ACCURACY train {train_accuracy}, valid {val_accuracy}')\n\n            # Early stopping\n            if val_average_loss < best_val_loss:\n                best_val_loss = val_average_loss\n                no_improvement_counter = 0\n                # Save the current best model\n                best_model_path = f'./GAT_model_lr_{LR}.pth'\n                torch.save({'epoch': epoch,\n                            'model_state': model.state_dict(),\n                            'optimizer_state_dict': optimizer.state_dict()}, best_model_path)\n            else:\n                no_improvement_counter += 1\n\n            if patience is not None and no_improvement_counter >= patience:\n                print(f\"No improvement for {patience} epochs. Early stopping...\")\n                break\n\n            end_time = timer()\n            print(f\"Epoch {epoch + 1} completed. Total epoch time: {end_time - start_time:.3f} seconds\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Define the test_model function\ndef test_model():\n    # Load the best saved model\n    checkpoint = torch.load(f'./GAT_model_lr_{LR}.pth')\n    model.load_state_dict(checkpoint['model_state'])\n    model.eval()\n\n    all_true_labels = []\n    all_predicted_labels = []\n    cum_test_loss = 0.0\n    correct_predictions_test = 0\n    total_samples_test = 0\n\n    with torch.no_grad():\n        for data_test in tqdm(test_loader):\n            data_test = data_test.to(device)\n\n            y_test_hat = model(data_test.x, data_test.edge_index, data_test.batch)\n\n            test_loss = criterion(y_test_hat, data_test.y)\n            cum_test_loss += test_loss.item()\n\n            _, predicted_test = torch.max(y_test_hat, 1)\n            correct_predictions_test += (predicted_test == data_test.y).sum().item()\n            total_samples_test += data_test.y.size(0)\n\n            # Collect all true labels and predictions\n            all_true_labels.extend(data_test.y.cpu().numpy())\n            all_predicted_labels.extend(predicted_test.cpu().numpy())\n\n        # Calculate accuracy and average loss for testing\n        test_accuracy = correct_predictions_test / total_samples_test\n        test_average_loss = cum_test_loss / total_samples_test\n\n        # Log test loss and accuracy to TensorBoard\n        writer.add_scalar('loss/test', test_average_loss, epoch + 1)\n        writer.add_scalar('acc/test', test_accuracy, epoch + 1)\n\n    print(f'LOSS test {test_average_loss}')\n    print(f'ACCURACY test {test_accuracy}')\n\n    # Generate classification report and confusion matrix\n    all_true_labels = np.array(all_true_labels)\n    all_predicted_labels = np.array(all_predicted_labels)\n    class_names = ['Imp_Surf', 'Bldgs', 'Low_Veg', 'Tree', 'Car', 'Clutter']\n\n    # Generate classification report\n    class_report = classification_report(all_true_labels, all_predicted_labels, target_names=class_names, zero_division=0)\n    print(\"Classification Report:\\n\", class_report)\n\n    # Generate confusion matrix\n    conf_matrix = confusion_matrix(all_true_labels, all_predicted_labels)\n\n    # Plot confusion matrix\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n    ax.set_xlabel('Predicted Labels')\n    ax.set_ylabel('True Labels')\n    ax.set_title('Confusion Matrix')\n    plt.show()\n\n    # Optionally log classification report and confusion matrix to TensorBoard\n    writer.add_text('Classification Report', class_report)\n    fig_conf_matrix = plt.figure(figsize=(10, 8))\n    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.title('Confusion Matrix')\n    writer.add_figure('Confusion Matrix', fig_conf_matrix)\n    plt.close(fig_conf_matrix)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:38:30.479050Z","iopub.execute_input":"2024-06-11T22:38:30.479416Z","iopub.status.idle":"2024-06-11T22:38:30.506338Z","shell.execute_reply.started":"2024-06-11T22:38:30.479391Z","shell.execute_reply":"2024-06-11T22:38:30.505361Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Call the train_model function\ntrain_model()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:38:34.011072Z","iopub.execute_input":"2024-06-11T22:38:34.011822Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the test_model function\ntest_model()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom tqdm import tqdm\n\ndef infer_model():\n    all_true_labels = []\n    all_predicted_labels = []\n    cum_test_loss = 0.0\n    correct_predictions_test = 0\n    total_samples_test = 0\n\n    # Load the best model\n    best_model_path = f'./GAT_model_lr_{LR}.pth'\n    checkpoint = torch.load(best_model_path, map_location=torch.device('cpu'))\n    model.load_state_dict(checkpoint['model_state'])\n    model.to(device)\n    model.eval()\n\n    with torch.no_grad():\n        for data_test in tqdm(test_loader):\n            data_test = data_test.to(device)\n\n            y_test_hat = model(data_test.x, data_test.edge_index, data_test.batch)\n\n            test_loss = criterion(y_test_hat, data_test.y)\n            cum_test_loss += test_loss.item()\n\n            _, predicted_test = torch.max(y_test_hat, 1)\n            correct_predictions_test += (predicted_test == data_test.y).sum().item()\n            total_samples_test += data_test.y.size(0)\n\n            # Collect all true labels and predictions\n            all_true_labels.extend(data_test.y.cpu().numpy())\n            all_predicted_labels.extend(predicted_test.cpu().numpy())\n\n    # Calculate accuracy and average loss for testing\n    test_accuracy = correct_predictions_test / total_samples_test\n    test_average_loss = cum_test_loss / total_samples_test\n\n    # Log test loss and accuracy to TensorBoard\n    writer.add_scalar('loss/test', test_average_loss, 0)\n    writer.add_scalar('acc/test', test_accuracy, 0)\n\n    print(f'LOSS test {test_average_loss}')\n    print(f'ACCURACY test {test_accuracy}')\n\n    # Generate classification report and confusion matrix\n    all_true_labels = np.array(all_true_labels)\n    all_predicted_labels = np.array(all_predicted_labels)\n    class_names = ['Imp_Surf', 'Bldgs', 'Low_Veg', 'Tree', 'Car', 'Clutter']\n\n    # Generate classification report\n    class_report = classification_report(all_true_labels, all_predicted_labels, target_names=class_names, zero_division=0)\n    print(\"Classification Report:\\n\", class_report)\n\n    # Generate confusion matrix\n    conf_matrix = confusion_matrix(all_true_labels, all_predicted_labels)\n\n    # Plot confusion matrix\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n    ax.set_xlabel('Predicted Labels')\n    ax.set_ylabel('True Labels')\n    ax.set_title('Confusion Matrix')\n    plt.show()\n\n    # Optionally log classification report and confusion matrix to TensorBoard\n    writer.add_text('Classification Report', class_report)\n    fig_conf_matrix = plt.figure(figsize=(10, 8))\n    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.title('Confusion Matrix')\n    writer.add_figure('Confusion Matrix', fig_conf_matrix)\n    plt.close(fig_conf_matrix)\n\n# Call the infer_model function\ninfer_model()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}