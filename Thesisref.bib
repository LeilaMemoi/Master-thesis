
@misc{danel_spatial_2020,
	title = {Spatial {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1909.05310},
	abstract = {Graph Convolutional Networks (GCNs) have recently become the primary choice for learning from graph-structured data, superseding hash ﬁngerprints in representing chemical compounds. However, GCNs lack the ability to take into account the ordering of node neighbors, even when there is a geometric interpretation of the graph vertices that provides an order based on their spatial positions. To remedy this issue, we propose Spatial Graph Convolutional Network (SGCN) which uses spatial features to eﬃciently learn from graphs that can be naturally located in space. Our contribution is threefold: we propose a GCN-inspired architecture which (i) leverages node positions, (ii) is a proper generalization of both GCNs and Convolutional Neural Networks (CNNs), (iii) beneﬁts from augmentation which further improves the performance and assures invariance with respect to the desired properties. Empirically, SGCN outperforms state-of-the-art graph-based methods on image classiﬁcation and chemical tasks.},
	language = {en},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Danel, Tomasz and Spurek, Przemysław and Tabor, Jacek and Śmieja, Marek and Struski, Łukasz and Słowik, Agnieszka and Maziarka, Łukasz},
	month = jul,
	year = {2020},
	note = {arXiv:1909.05310 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{spurek_geometric_nodate,
	title = {Geometric {Graph} {Convolutional} {Neural} {Networks}},
	abstract = {Graph Convolutional Networks (GCNs) have recently become the primary choice for learning from graph-structured data, superseding hash ﬁngerprints in representing chemical compounds. However, GCNs lack the ability to take into account the ordering of node neighbors, even when there is a geometric interpretation of the graph vertices that provides an order based on their spatial positions. To remedy this issue, we propose Geometric Graph Convolutional Network (geoGCN) which uses spatial features to efﬁciently learn from graphs that can be naturally located in space. Our contribution is threefold: we propose a GCN-inspired architecture which (i) leverages node positions, (ii) is a proper generalisation of both GCNs and Convolutional Neural Networks (CNNs), (iii) beneﬁts from augmentation which further improves the performance and assures invariance with respect to the desired properties. Empirically, geo-GCN outperforms state-of-theart graph-based methods on image classiﬁcation and chemical tasks.},
	language = {en},
	author = {Spurek, Przemysław and Danel, Tomasz and Tabor, Jacek and Struski, Łukasz and Słowik, Agnieszka and Maziarka, Łukasz},
}

@article{spurek_geometric_nodate-1,
	title = {Geometric {Graph} {Convolutional} {Neural} {Networks}},
	abstract = {Graph Convolutional Networks (GCNs) have recently become the primary choice for learning from graph-structured data, superseding hash ﬁngerprints in representing chemical compounds. However, GCNs lack the ability to take into account the ordering of node neighbors, even when there is a geometric interpretation of the graph vertices that provides an order based on their spatial positions. To remedy this issue, we propose Geometric Graph Convolutional Network (geoGCN) which uses spatial features to efﬁciently learn from graphs that can be naturally located in space. Our contribution is threefold: we propose a GCN-inspired architecture which (i) leverages node positions, (ii) is a proper generalisation of both GCNs and Convolutional Neural Networks (CNNs), (iii) beneﬁts from augmentation which further improves the performance and assures invariance with respect to the desired properties. Empirically, geo-GCN outperforms state-of-theart graph-based methods on image classiﬁcation and chemical tasks.},
	language = {en},
	author = {Spurek, Przemysław and Danel, Tomasz and Tabor, Jacek and Struski, Łukasz and Słowik, Agnieszka and Maziarka, Łukasz},
}

@misc{hamilton_inductive_2018,
	title = {Inductive {Representation} {Learning} on {Large} {Graphs}},
	url = {http://arxiv.org/abs/1706.02216},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efﬁciently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node’s local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classiﬁcation benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	language = {en},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	month = sep,
	year = {2018},
	note = {arXiv:1706.02216 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@misc{velickovic_graph_2018,
	title = {Graph {Attention} {Networks}},
	url = {http://arxiv.org/abs/1710.10903},
	abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods’ features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).},
	language = {en},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
	month = feb,
	year = {2018},
	note = {arXiv:1710.10903 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@article{ming-kuei_hu_visual_1962,
	title = {Visual pattern recognition by moment invariants},
	volume = {8},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1057692/},
	doi = {10.1109/TIT.1962.1057692},
	abstract = {In this paper a theory of two-dimensional moment invariants for planar geometric figures is presented. A fundamental theorem is established to relate such moment invariants to the wellknown algebraic invariants. Complete systems of moment invariants under translation, similitude and orthogonal transformations are derived. Some moment invariants under general two-dimensional linear transformations are also included. Both theoretical formulation and practical models of visual pattern recognition based upon these moment invariants are discussed. A simple simulation program together with its performance are also presented. It is shown that recognition of geometrical patterns and alphabetical characters independently of position, size and orientation can be accomplished. It is also indicated that generalization is possible to include invariance with parallel projection.},
	language = {en},
	number = {2},
	urldate = {2024-05-24},
	journal = {IEEE Transactions on Information Theory},
	author = {{Ming-Kuei Hu}},
	month = feb,
	year = {1962},
	pages = {179--187},
}

@article{lu_object-based_2024,
	title = {Object-{Based} {Semi}-{Supervised} {Spatial} {Attention} {Residual} {UNet} for {Urban} {High}-{Resolution} {Remote} {Sensing} {Image} {Classification}},
	volume = {16},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/16/8/1444},
	doi = {10.3390/rs16081444},
	abstract = {Accurate urban land cover information is crucial for effective urban planning and management. While convolutional neural networks (CNNs) demonstrate superior feature learning and prediction capabilities using image-level annotations, the inherent mixed-category nature of input image patches leads to classification errors along object boundaries. Fully convolutional neural networks (FCNs) excel at pixel-wise fine segmentation, making them less susceptible to heterogeneous content, but they require fully annotated dense image patches, which may not be readily available in real-world scenarios. This paper proposes an object-based semi-supervised spatial attention residual UNet (OS-ARU) model. First, multiscale segmentation is performed to obtain segments from a remote sensing image, and segments containing sample points are assigned the categories of the corresponding points, which are used to train the model. Then, the trained model predicts class probabilities for all segments. Each unlabeled segment’s probability distribution is compared against those of labeled segments for similarity matching under a threshold constraint. Through label propagation, pseudo-labels are assigned to unlabeled segments exhibiting high similarity to labeled ones. Finally, the model is retrained using the augmented training set incorporating the pseudo-labeled segments. Comprehensive experiments on aerial image benchmarks for Vaihingen and Potsdam demonstrate that the proposed OS-ARU achieves higher classification accuracy than state-of-the-art models, including OCNN, 2OCNN, and standard OS-U, reaching an overall accuracy (OA) of 87.83\% and 86.71\%, respectively. The performance improvements over the baseline methods are statistically significant according to the Wilcoxon Signed-Rank Test. Despite using significantly fewer sparse annotations, this semi-supervised approach still achieves comparable accuracy to the same model under full supervision. The proposed method thus makes a step forward in substantially alleviating the heavy sampling burden of FCNs (densely sampled deep learning models) to effectively handle the complex issue of land cover information identification and classification.},
	language = {en},
	number = {8},
	urldate = {2024-05-24},
	journal = {Remote Sensing},
	author = {Lu, Yuanbing and Li, Huapeng and Zhang, Ce and Zhang, Shuqing},
	month = apr,
	year = {2024},
	pages = {1444},
}

@article{hao_land-use_2024,
	title = {Land-use classification based on high-resolution remote sensing imagery and deep learning models},
	volume = {19},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0300473},
	doi = {10.1371/journal.pone.0300473},
	abstract = {High-resolution imagery and deep learning models have gained increasing importance in land-use mapping. In recent years, several new deep learning network modeling methods have surfaced. However, there has been a lack of a clear understanding of the performance of these models. In this study, we applied four well-established and robust deep learning models (FCN-8s, SegNet, U-Net, and Swin-UNet) to an open benchmark high-resolution remote sensing dataset to compare their performance in land-use mapping. The results indicate that FCN-8s, SegNet, U-Net, and Swin-UNet achieved overall accuracies of 80.73\%, 89.86\%, 91.90\%, and 96.01\%, respectively, on the test set. Furthermore, we assessed the generalization ability of these models using two measures: intersection of union and F1 score, which highlight Swin-UNet’s superior robustness compared to the other three models. In summary, our study provides a systematic analysis of the classification differences among these four deep learning models through experiments. It serves as a valuable reference for selecting models in future research, particularly in scenarios such as land-use mapping, urban functional area recognition, and natural resource management.},
	language = {en},
	number = {4},
	urldate = {2024-05-24},
	journal = {PLOS ONE},
	author = {Hao, Mengmeng and Dong, Xiaohan and Jiang, Dong and Yu, Xianwen and Ding, Fangyu and Zhuo, Jun},
	editor = {Bilal, Anas},
	month = apr,
	year = {2024},
	pages = {e0300473},
}

@article{sheykhmousa_support_2020,
	title = {Support {Vector} {Machine} {Versus} {Random} {Forest} for {Remote} {Sensing} {Image} {Classification}: {A} {Meta}-{Analysis} and {Systematic} {Review}},
	volume = {13},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {1939-1404, 2151-1535},
	shorttitle = {Support {Vector} {Machine} {Versus} {Random} {Forest} for {Remote} {Sensing} {Image} {Classification}},
	url = {https://ieeexplore.ieee.org/document/9206124/},
	doi = {10.1109/JSTARS.2020.3026724},
	abstract = {Several machine-learning algorithms have been proposed for remote sensing image classiﬁcation during the past two decades. Among these machine learning algorithms, Random Forest (RF) and Support Vector Machines (SVM) have drawn attention to image classiﬁcation in several remote sensing applications. This article reviews RF and SVM concepts relevant to remote sensing image classiﬁcation and applies a meta-analysis of 251 peer-reviewed journal papers. A database with more than 40 quantitative and qualitative ﬁelds was constructed from these reviewed papers. The meta-analysis mainly focuses on 1) the analysis regarding the general characteristics of the studies, such as geographical distribution, frequency of the papers considering time, journals, application domains, and remote sensing software packages used in the case studies, and 2) a comparative analysis regarding the performances of RF and SVM classiﬁcation against various parameters, such as data type, RS applications, spatial resolution, and the number of extracted features in the feature engineering step. The challenges, recommendations, and potential directions for future research are also discussed in detail. Moreover, a summary of the results is provided to aid researchers to customize their efforts in order to achieve the most accurate results based on their thematic applications.},
	language = {en},
	urldate = {2024-05-20},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Sheykhmousa, Mohammadreza and Mahdianpari, Masoud and Ghanbari, Hamid and Mohammadimanesh, Fariba and Ghamisi, Pedram and Homayouni, Saeid},
	year = {2020},
	pages = {6308--6325},
}

@article{zhou_graph_2020,
	title = {Graph neural networks: {A} review of methods and applications},
	volume = {1},
	issn = {26666510},
	shorttitle = {Graph neural networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666651021000012},
	doi = {10.1016/j.aiopen.2021.01.001},
	abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular ﬁngerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.},
	language = {en},
	urldate = {2024-05-14},
	journal = {AI Open},
	author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	year = {2020},
	pages = {57--81},
}

@misc{noauthor_how_nodate,
	title = {How powerful are {Graph} {Convolutional} {Networks}?},
	url = {http://tkipf.github.io/graph-convolutional-networks/},
	abstract = {Many important real-world datasets come in the form of graphs or networks: social networks, knowledge graphs, protein-interaction networks, the World Wide Web, etc. (just to name a few). Yet, until recently, very little attention has been devoted to the generalization of neural...},
	urldate = {2024-05-13},
}

@misc{noauthor_tutorial_nodate,
	title = {Tutorial 6: {Basics} of {Graph} {Neural} {Networks} — {PyTorch} {Lightning} 2.2.3 documentation},
	url = {https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/06-graph-neural-networks.html},
	urldate = {2024-05-13},
}

@misc{noauthor_understanding_nodate,
	title = {Understanding {Convolutions} on {Graphs}},
	url = {https://distill.pub/2021/understanding-gnns/},
	urldate = {2024-05-11},
}

@article{scarselli_graph_2009,
	title = {The {Graph} {Neural} {Network} {Model}},
	volume = {20},
	issn = {1941-0093},
	url = {https://ieeexplore.ieee.org/document/4700287},
	doi = {10.1109/TNN.2008.2005605},
	abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
	number = {1},
	urldate = {2024-05-11},
	journal = {IEEE Transactions on Neural Networks},
	author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
	month = jan,
	year = {2009},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Biological system modeling, Biology, Chemistry, Computer vision, Data engineering, Data mining, Graphical domains, Neural networks, Parameter estimation, Pattern recognition, Supervised learning, graph neural networks (GNNs), graph processing, recursive neural networks},
	pages = {61--80},
}

@book{wilson_introduction_2009,
	address = {Harlow Munich},
	edition = {4. ed., [Nachdr.]},
	title = {Introduction to graph theory},
	isbn = {978-0-582-24993-6},
	language = {en},
	publisher = {Prentice Hall},
	author = {Wilson, Robin J.},
	year = {2009},
}

@incollection{corsini_graphs_2003,
	address = {Boston, MA},
	title = {Graphs and {Hypergraphs}},
	isbn = {978-1-4757-3714-1},
	url = {https://doi.org/10.1007/978-1-4757-3714-1_3},
	abstract = {Since the middle of the last century, Graph Theory has been an important tool in different fields, Iike Geometry, Algebra, Number Theory, Topology, Optimization, Operations Research, Median Algebras and so on. To solve new combinatorial problems, it was necessary to generalize the concept of a Graph.},
	language = {en},
	urldate = {2024-05-11},
	booktitle = {Applications of {Hyperstructure} {Theory}},
	publisher = {Springer US},
	author = {Corsini, Piergiulio and Leoreanu, Violeta},
	editor = {Corsini, Piergiulio and Leoreanu, Violeta},
	year = {2003},
	doi = {10.1007/978-1-4757-3714-1_3},
	pages = {55--94},
}

@misc{xu_how_2019,
	title = {How {Powerful} are {Graph} {Neural} {Networks}?},
	url = {http://arxiv.org/abs/1810.00826},
	doi = {10.48550/arXiv.1810.00826},
	abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
	urldate = {2024-05-11},
	publisher = {arXiv},
	author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
	month = feb,
	year = {2019},
	note = {arXiv:1810.00826 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sanchez-lengeling_gentle_2021,
	title = {A {Gentle} {Introduction} to {Graph} {Neural} {Networks}},
	volume = {6},
	issn = {2476-0757},
	url = {https://distill.pub/2021/gnn-intro},
	doi = {10.23915/distill.00033},
	abstract = {What components are needed for building learning algorithms that leverage the structure and properties of graphs?},
	language = {en},
	number = {9},
	urldate = {2024-05-11},
	journal = {Distill},
	author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
	month = sep,
	year = {2021},
	pages = {e33},
}

@misc{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efﬁcient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized ﬁrst-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a signiﬁcant margin.},
	language = {en},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv:1609.02907 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{casalegno_graph_2021,
	title = {Graph {Convolutional} {Networks}},
	url = {https://towardsdatascience.com/graph-convolutional-networks-deep-99d7fee5706f},
	abstract = {Graph applications are ubiquitous, learn how to use Machine Learning with them!},
	language = {en},
	urldate = {2024-05-10},
	journal = {Medium},
	author = {Casalegno, Francesco},
	month = feb,
	year = {2021},
}

@article{daigavane_understanding_2021,
	title = {Understanding {Convolutions} on {Graphs}},
	volume = {6},
	issn = {2476-0757},
	url = {https://distill.pub/2021/understanding-gnns},
	doi = {10.23915/distill.00032},
	abstract = {Understanding the building blocks and design choices of graph neural networks.},
	language = {en},
	number = {9},
	urldate = {2024-05-10},
	journal = {Distill},
	author = {Daigavane, Ameya and Ravindran, Balaraman and Aggarwal, Gaurav},
	month = sep,
	year = {2021},
	pages = {e32},
}

@inproceedings{porebski_haralick_2008,
	title = {Haralick feature extraction from {LBP} images for color texture classification},
	url = {https://ieeexplore.ieee.org/abstract/document/4743780?casa_token=9sYGVBkRESIAAAAA:2qvUWIfIBINPru92JigEesRjiOcyZ7qNVt0XpaeVa34nIxKPRrSVTKYxgPxI7-KpExK-8CCiag},
	doi = {10.1109/IPTA.2008.4743780},
	abstract = {In this paper, we present a new approach for color texture classification by use of Haralick features extracted from co-occurrence matrices computed from local binary pattern (LBP) images. These LBP images, which are different from the color LBP initially proposed by Maenpaa and Pietikainen, are extracted from color texture images, which are coded in 28 different color spaces. An iterative procedure then selects among the extracted features, those which discriminate the textures, in order to build a low dimensional feature space. Experimental results, achieved with the BarkTex database, show the interest of this method with which a satisfying rate of well-classified images (85.6\%) is obtained, with a 10-dimensional feature space.},
	urldate = {2024-05-07},
	booktitle = {2008 {First} {Workshops} on {Image} {Processing} {Theory}, {Tools} and {Applications}},
	author = {Porebski, Alice and Vandenbroucke, Nicolas and Macaire, Ludovic},
	month = nov,
	year = {2008},
	note = {ISSN: 2154-512X},
	keywords = {Color texture classification, Electronic mail, Feature extraction, Image analysis, Image color analysis, Image databases, Image processing, Image texture analysis, Industrial control, LBP images, Quality control, Spatial databases},
	pages = {1--8},
}

@article{barburiceanu_3d_2021,
	title = {{3D} {Texture} {Feature} {Extraction} and {Classification} {Using} {GLCM} and {LBP}-{Based} {Descriptors}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/5/2332},
	doi = {10.3390/app11052332},
	abstract = {Lately, 3D imaging techniques have achieved a lot of progress due to recent developments in 3D sensor technologies. This leads to a great interest regarding 3D image feature extraction and classification techniques. As pointed out in literature, one of the most important and discriminative features in images is the textural content. Within this context, we propose a texture feature extraction technique for volumetric images with improved discrimination power. The method could be used in textured volumetric data classification tasks. To achieve this, we fuse two complementary pieces of information, feature vectors derived from Local Binary Patterns (LBP) and the Gray-Level Co-occurrence Matrix-based methods. They provide information regarding the image pattern and the contrast, homogeneity and local anisotropy in the volumetric data, respectively. The performance of the proposed technique was evaluated on a public dataset consisting of volumetric textured images affected by several transformations. The classifiers used are the Support Vector Machine, k-Nearest Neighbours and Random Forest. Our method outperforms other handcrafted 3D or 2D texture feature extraction methods and typical deep-learning networks. The proposed technique improves the discrimination power and achieves promising results even if the number of images per class is relatively small.},
	language = {en},
	number = {5},
	urldate = {2024-05-07},
	journal = {Applied Sciences},
	author = {Barburiceanu, Stefania and Terebes, Romulus and Meza, Serban},
	month = jan,
	year = {2021},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D co-occurrence matrix, 3D feature extraction, GLCM, LBP, volumetric texture classification},
	pages = {2332},
}

@article{raju_texture_2008,
	title = {Texture {Description} using {Different} {Wavelet} {Transforms} based on {Statistical} {Parameters}},
	abstract = {The present paper estimates the success rates of Original, Haar, Daubechies-6 and Coiflet-6 wavelet transforms based on first order statistics. Statistical approach is one of the best ways to describe texture primitives. First order statistics are very straight forward. They are calculated from the probability of observing a particular pixel value at a randomly chosen location in the image. The present paper estimated the first order statistics on entire image to estimate the overall behavior of texture with respect to their primitives. In this paper, texture description based on first order statistical features obtained from various one level wavelet transforms are proposed. The various wavelets considered are Haar, Daubachies-6 and Coiflet-6. Since the most significant information of a texture often appears in the approximation coefficients part, this part is used for the computation of first order statistical parameters. Further texture descriptive rates of these wavelets, based on their success rates, were evaluated with the comparison of original textures. The experimental results on 24 Brodatz textures have given good results and concise conclusions are drawn.},
	language = {en},
	journal = {Signal Processing},
	author = {Raju, U S N and Kumar, V Vijaya and Suresh, A and Mani, M Radhika},
	year = {2008},
}

@misc{noauthor_notitle_nodate,
}

@article{noauthor_notitle_nodate-1,
}

@article{grigorescu_comparison_2002,
	title = {Comparison of texture features based on {Gabor} filters},
	volume = {11},
	issn = {1941-0042},
	url = {https://ieeexplore.ieee.org/abstract/document/1042386?casa_token=FRm1OWC1r5QAAAAA:JoVGYzU15NWlJuzJfzzt68UxK7IUdg5FrRnOZwcbzfJj-qMmVIIOsao73RVP-_31uA4SDwIQZA},
	doi = {10.1109/TIP.2002.804262},
	abstract = {Texture features that are based on the local power spectrum obtained by a bank of Gabor filters are compared. The features differ in the type of nonlinear post-processing which is applied to the local power spectrum. The following features are considered: Gabor energy, complex moments, and grating cell operator features. The capability of the corresponding operators to produce distinct feature vector clusters for different textures is compared using two methods: the Fisher (1923) criterion and the classification result comparison. Both methods give consistent results. The grating cell operator gives the best discrimination and segmentation results. The texture detection capabilities of the operators and their robustness to nontexture features are also compared. The grating cell operator is the only one that selectively responds only to texture and does not give false response to nontexture features such as object contours.},
	number = {10},
	urldate = {2024-05-07},
	journal = {IEEE Transactions on Image Processing},
	author = {Grigorescu, S.E. and Petkov, N. and Kruizinga, P.},
	month = oct,
	year = {2002},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Band pass filters, Filtering, Frequency domain analysis, Gabor filters, Gratings, Image segmentation, Image texture analysis, Nonlinear filters, Robustness, Two dimensional displays},
	pages = {1160--1167},
}

@article{petrovska_deep_2020,
	title = {Deep {Learning} for {Feature} {Extraction} in {Remote} {Sensing}: {A} {Case}-{Study} of {Aerial} {Scene} {Classification}},
	volume = {20},
	issn = {1424-8220},
	shorttitle = {Deep {Learning} for {Feature} {Extraction} in {Remote} {Sensing}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7411945/},
	doi = {10.3390/s20143906},
	abstract = {Scene classification relying on images is essential in many systems and applications related to remote sensing. The scientific interest in scene classification from remotely collected images is increasing, and many datasets and algorithms are being developed. The introduction of convolutional neural networks (CNN) and other deep learning techniques contributed to vast improvements in the accuracy of image scene classification in such systems. To classify the scene from areal images, we used a two-stream deep architecture. We performed the first part of the classification, the feature extraction, using pre-trained CNN that extracts deep features of aerial images from different network layers: the average pooling layer or some of the previous convolutional layers. Next, we applied feature concatenation on extracted features from various neural networks, after dimensionality reduction was performed on enormous feature vectors. We experimented extensively with different CNN architectures, to get optimal results. Finally, we used the Support Vector Machine (SVM) for the classification of the concatenated features. The competitiveness of the examined technique was evaluated on two real-world datasets: UC Merced and WHU-RS. The obtained classification accuracies demonstrate that the considered method has competitive results compared to other cutting-edge techniques.},
	number = {14},
	urldate = {2024-05-07},
	journal = {Sensors (Basel, Switzerland)},
	author = {Petrovska, Biserka and Zdravevski, Eftim and Lameski, Petre and Corizzo, Roberto and Štajduhar, Ivan and Lerga, Jonatan},
	month = jul,
	year = {2020},
	pmid = {32674254},
	pmcid = {PMC7411945},
	pages = {3906},
}

@misc{noauthor_fully_nodate,
	title = {Fully convolutional networks for semantic segmentation {\textbar} {IEEE} {Conference} {Publication} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/7298965},
	urldate = {2024-03-19},
}

@misc{noauthor_colab_nodate,
	title = {Colab {Notebooks} and {Video} {Tutorials} — pytorch\_geometric documentation},
	url = {https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html},
	urldate = {2024-03-18},
}

@article{diao_superpixel-based_2022,
	title = {Superpixel-{Based} {Attention} {Graph} {Neural} {Network} for {Semantic} {Segmentation} in {Aerial} {Images}},
	volume = {14},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/2/305},
	doi = {10.3390/rs14020305},
	abstract = {Semantic segmentation is one of the signiﬁcant tasks in understanding aerial images with high spatial resolution. Recently, Graph Neural Network (GNN) and attention mechanism have achieved excellent performance in semantic segmentation tasks in general images and been applied to aerial images. In this paper, we propose a novel Superpixel-based Attention Graph Neural Network (SAGNN) for semantic segmentation of high spatial resolution aerial images. A K-Nearest Neighbor (KNN) graph is constructed from our network for each image, where each node corresponds to a superpixel in the image and is associated with a hidden representation vector. On this basis, the initialization of the hidden representation vector is the appearance feature extracted by a unary Convolutional Neural Network (CNN) from the image. Moreover, relying on the attention mechanism and recursive functions, each node can update its hidden representation according to the current state and the incoming information from its neighbors. The ﬁnal representation of each node is used to predict the semantic class of each superpixel. The attention mechanism enables graph nodes to differentially aggregate neighbor information, which can extract higher-quality features. Furthermore, the superpixels not only save computational resources, but also maintain object boundary to achieve more accurate predictions. The accuracy of our model on the Potsdam and Vaihingen public datasets exceeds all benchmark approaches, reaching 90.23\% and 89.32\%, respectively.},
	language = {en},
	number = {2},
	urldate = {2024-03-18},
	journal = {Remote Sensing},
	author = {Diao, Qi and Dai, Yaping and Zhang, Ce and Wu, Yan and Feng, Xiaoxue and Pan, Feng},
	month = jan,
	year = {2022},
	pages = {305},
}

@article{chen_adaptive_2020,
	title = {Adaptive {Effective} {Receptive} {Field} {Convolution} for {Semantic} {Segmentation} of {VHR} {Remote} {Sensing} {Images}},
	volume = {PP},
	doi = {10.1109/TGRS.2020.3009143},
	abstract = {Convolutional neural networks (CNNs) have facilitated impressive improvements in the semantic segmentation of very high-resolution (VHR) remote sensing images. The success of semantic segmentation depends on an effective receptive field (RF) large enough to cover the entire object. Popular methods to enlarge the effective RF include dilated filters, subsampling operations, and stacking layers. Unfortunately, the methods are inefficient or able to cause grid artifacts. Moreover, although the object sizes vary greatly in remote sensing images, the size of the RF cannot reach a compromise between small and large objects. To tackle these problems, we propose adaptive effective receptive convolution (AERFC) for VHR remote sensing images. AERFC adaptively controls the sampling location of convolution and automatically adjusts the effective RF without significantly increasing the parameter number and computational cost. Thus, AERFC reduces the training difficulty, decreases overfitting risk, and reserves details in VHR images. AERFC is also integrated with spatial pyramid pooling (SPP) to aggregate diverse multiscale features for exploring contextual information. Experimental results of the quantitative and qualitative evaluation over four benchmark data sets show that AERFC outperforms state-of-the-art methods.},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Chen, Xi and Li, Zhiqiang and Jiang, Jie and Han, Zhen and Deng, Shiyi and Li, Zhihong and Fang, Tao and Huo, Hong and Li, Qingli and Liu, Min},
	month = jul,
	year = {2020},
	pages = {1--15},
}

@article{tiwari_feature_2013,
	title = {Feature {Extraction} for {Object} {Recognition} and {Image} {Classification}},
	volume = {2},
	abstract = {Feature Extraction is one of the most popular research areas in the field of image analysis as it is a prime requirement in order to represent an object. An object is represented by a group of features in form of a feature vector. This feature vector is used to recognize objects and classify them. Previous works have proposed various feature extraction techniques to find the feature vector. This paper provides a comprehensive framework of various feature extraction techniques and their use in object recognition and classification. It also provides their comparison. Various techniques have been considered and their pros and cons along with the method of implementation and detailed experimental results have been discussed.},
	language = {en},
	number = {10},
	journal = {International Journal of Engineering Research},
	author = {Tiwari, Aastha and Goswami, Anil Kumar and Saraswat, Mansi},
	year = {2013},
}

@misc{avelar_superpixel_2020,
	title = {Superpixel {Image} {Classification} with {Graph} {Attention} {Networks}},
	url = {http://arxiv.org/abs/2002.05544},
	doi = {10.48550/arXiv.2002.05544},
	abstract = {This paper presents a methodology for image classification using Graph Neural Network (GNN) models. We transform the input images into region adjacency graphs (RAGs), in which regions are superpixels and edges connect neighboring superpixels. Our experiments suggest that Graph Attention Networks (GATs), which combine graph convolutions with self-attention mechanisms, outperforms other GNN models. Although raw image classifiers perform better than GATs due to information loss during the RAG generation, our methodology opens an interesting avenue of research on deep learning beyond rectangular-gridded images, such as 360-degree field of view panoramas. Traditional convolutional kernels of current state-of-the-art methods cannot handle panoramas, whereas the adapted superpixel algorithms and the resulting region adjacency graphs can naturally feed a GNN, without topology issues.},
	urldate = {2024-02-15},
	publisher = {arXiv},
	author = {Avelar, Pedro H. C. and Tavares, Anderson R. and da Silveira, Thiago L. T. and Jung, Cláudio R. and Lamb, Luís C.},
	month = nov,
	year = {2020},
	note = {arXiv:2002.05544 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{jia_graph--graph_2024,
	title = {Graph-in-{Graph} {Convolutional} {Network} for {Hyperspectral} {Image} {Classification}},
	volume = {35},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/document/9801664},
	doi = {10.1109/TNNLS.2022.3182715},
	abstract = {With the development of hyperspectral sensors, accessible hyperspectral images (HSIs) are increasing, and pixel-oriented classification has attracted much attention. Recently, graph convolutional networks (GCNs) have been proposed to process graph-structured data in non-Euclidean domains and have been employed in HSI classification. But most methods based on GCN are hard to sufficiently exploit information of ground objects due to feature aggregation. To solve this issue, in this article, we proposed a graph-in-graph (GiG) model and a related GiG convolutional network (GiGCN) for HSI classification from a superpixel viewpoint. The GiG representation covers information inside and outside superpixels, respectively, corresponding to the local and global characteristics of ground objects. Concretely, after segmenting HSI into disjoint superpixels, each one is converted to an internal graph. Meanwhile, an external graph is constructed according to the spatial adjacent relationships among superpixels. Significantly, each node in the external graph embeds a corresponding internal graph, forming the so-called GiG structure. Then, GiGCN composed of internal and External graph convolution (EGC) is designed to extract hierarchical features and integrate them into multiple scales, improving the discriminability of GiGCN. Ensemble learning is incorporated to further boost the robustness of GiGCN. It is worth noting that we are the first to propose the GiG framework from the superpixel point and the GiGCN scheme for HSI classification. Experiment results on four benchmark datasets demonstrate that our proposed method is effective and feasible for HSI classification with limited labeled samples. For study replication, the code developed for this study is available at https://github.com/ShuGuoJ/GiGCN.git.},
	number = {1},
	urldate = {2024-01-22},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Jia, Sen and Jiang, Shuguo and Zhang, Shuyu and Xu, Meng and Jia, Xiuping},
	month = jan,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	pages = {1157--1171},
}

@article{cao_hierarchical_2024,
	title = {Hierarchical structural graph neural network with local relation enhancement for hyperspectral image classification},
	volume = {146},
	issn = {1051-2004},
	url = {https://www.sciencedirect.com/science/article/pii/S1051200424000174},
	doi = {10.1016/j.dsp.2024.104392},
	abstract = {In recent years, graph convolutional networks (GCNs) have made remarkable achievements in the hyperspectral image (HSI) classification task. However, existing GCN-based methods cannot adequately encode similarity edge relationship between superpixels, and few of them use hierarchical mechanism to extract complementary features. This paper addresses these issues and proposes a hierarchical structural graph neural network with local relation enhancement (HSLRE) for HSI classification. Specifically, the features of the pixel-level graph structures are extracted and then embedded into the superpixel-level graph structure to ensure that it does not lose the fine texture features of the original HSI. Secondly, a novel hierarchical framework, which consists of multiple coarsening and refining stages, is proposed to extract multi-level features. In the first coarsening stage, the relational graph convolution (RGC) is introduced to enhance local relations and obtain discriminative features from the superpixel-level graph. In the subsequent coarsening stages, graph convolution (GC) is used to extract features. The refining stages correspond to the coarsening stages, which are used to restore the graphs to their original structures. Finally, to enhance the fluidity of feature information, the fully connected layers and two different types of graph convolutional layers are utilized to extract the linear and nonlinear features of the nodes in parallel, which are fused in a weighted way to form effective features. Experimental results on several benchmark HSI datasets illustrate the effectiveness of the HSLRE.},
	urldate = {2024-01-22},
	journal = {Digital Signal Processing},
	author = {Cao, Feilong and Huang, Xiaomei and Yang, Bing and Ye, Hailiang},
	month = mar,
	year = {2024},
	keywords = {Graph convolutional network, Hierarchical framework, Hyperspectral image classification, Relational graph convolution},
	pages = {104392},
}

@article{velickovic_everything_2023,
	title = {Everything is connected: {Graph} neural networks},
	volume = {79},
	issn = {0959-440X},
	shorttitle = {Everything is connected},
	url = {https://www.sciencedirect.com/science/article/pii/S0959440X2300012X},
	doi = {10.1016/j.sbi.2023.102538},
	abstract = {In many ways, graphs are the main modality of data we receive from nature. This is due to the fact that most of the patterns we see, both in natural and artificial systems, are elegantly representable using the language of graph structures. Prominent examples include molecules (represented as graphs of atoms and bonds), social networks and transportation networks. This potential has already been seen by key scientific and industrial groups, with already-impacted application areas including traffic forecasting, drug discovery, social network analysis and recommender systems. Further, some of the most successful domains of application for machine learning in previous years—images, text and speech processing—can be seen as special cases of graph representation learning, and consequently there has been significant exchange of information between these areas. The main aim of this short survey is to enable the reader to assimilate the key concepts in the area, and position graph representation learning in a proper context with related fields.},
	urldate = {2024-01-13},
	journal = {Current Opinion in Structural Biology},
	author = {Veličković, Petar},
	month = apr,
	year = {2023},
	keywords = {Drug discovery, GNNs, Graph neural networks, Graph representation learning, Traffic forecasting, Transformers},
	pages = {102538},
}

@misc{noauthor_sensors_nodate,
	title = {Sensors {\textbar} {Free} {Full}-{Text} {\textbar} {Graph} {Neural} {Network}-{Based} {Method} of {Spatiotemporal} {Land} {Cover} {Mapping} {Using} {Satellite} {Imagery}},
	url = {https://www.mdpi.com/1424-8220/23/14/6648},
	urldate = {2024-01-13},
}

@article{stutz_superpixels_2018,
	title = {Superpixels: {An} evaluation of the state-of-the-art},
	volume = {166},
	issn = {1077-3142},
	shorttitle = {Superpixels},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314217300589},
	doi = {10.1016/j.cviu.2017.03.007},
	abstract = {Superpixels group perceptually similar pixels to create visually meaningful entities while heavily reducing the number of primitives for subsequent processing steps. As of these properties, superpixel algorithms have received much attention since their naming in 2003 (Ren and Malik, 2003). By today, publicly available superpixel algorithms have turned into standard tools in low-level vision. As such, and due to their quick adoption in a wide range of applications, appropriate benchmarks are crucial for algorithm selection and comparison. Until now, the rapidly growing number of algorithms as well as varying experimental setups hindered the development of a unifying benchmark. We present a comprehensive evaluation of 28 state-of-the-art superpixel algorithms utilizing a benchmark focussing on fair comparison and designed to provide new insights relevant for applications. To this end, we explicitly discuss parameter optimization and the importance of strictly enforcing connectivity. Furthermore, by extending well-known metrics, we are able to summarize algorithm performance independent of the number of generated superpixels, thereby overcoming a major limitation of available benchmarks. Furthermore, we discuss runtime, robustness against noise, blur and affine transformations, implementation details as well as aspects of visual quality. Finally, we present an overall ranking of superpixel algorithms which redefines the state-of-the-art and enables researchers to easily select appropriate algorithms and the corresponding implementations which themselves are made publicly available as part of our benchmark at http://www.davidstutz.de/projects/superpixel-benchmark/.},
	urldate = {2024-01-12},
	journal = {Computer Vision and Image Understanding},
	author = {Stutz, David and Hermans, Alexander and Leibe, Bastian},
	month = jan,
	year = {2018},
	keywords = {Benchmark, Evaluation, Image segmentation, Perceptual grouping, Superpixel segmentation, Superpixels},
	pages = {1--27},
}

@article{rottensteiner_isprs_2012,
	title = {The {ISPRS} benchmark on urban object classification and {3D} building reconstruction},
	volume = {I-3},
	doi = {10.5194/isprsannals-I-3-293-2012},
	abstract = {For more than two decades, many efforts have been made to develop methods for extracting urban objects from data acquired by
airborne sensors. In order to make the results of such algorithms more comparable, benchmarking data sets are of paramount
importance. Such a data set, consisting of airborne image and laserscanner data, has been made available to the scientific
community. Researchers were encouraged to submit results of urban object detection and 3D building reconstruction, which were
evaluated based on reference data. This paper presents the outcomes of the evaluation for building detection, tree detection, and 3D
building reconstruction. The results achieved by different methods are compared and analysed to identify promising strategies for
automatic urban object extraction from current airborne sensor data, but also common problems of state-of-the-art methods.},
	journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Rottensteiner, Franz and Sohn, Gunho and Jung, Jaewook and Gerke, Markus and Baillard, Caroline and Bénitez, Sébastien and Breitkopf, U},
	month = jul,
	year = {2012},
}

@article{ibrahim_automatic_2014,
	title = {Automatic {Quick}-{Shift} {Segmentation} for {Color} {Images}},
	volume = {11},
	abstract = {This paper proposes an automatic quick-shift segmentation method using illumination invariant representation of natural color images. In practice, the quick-shift segmentation method is sensitive to the choice of parameters, thus a quick tuning by hand is not sufficient. The proposed method segments images into homogeneous regions by applying the quick-shift method with initial values, then changes the quick-shift parameters values automatically to get the final segmented image. Changing parameters values make the proposed method flexible and robust against different image characteristics. We eliminate the factors that may affect natural image acquisition such as shadow and highlight by applying an invariant method. This method is valid for large size images. The effectiveness of the proposed method for a variety of images including different objects of metals and dielectrics is examined in experiments.},
	language = {en},
	number = {3},
	author = {Ibrahim, Abdelhameed and Salem, Muhammed and Ali, Hesham Arafat},
	year = {2014},
}

@inproceedings{vedaldi_quick_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Quick {Shift} and {Kernel} {Methods} for {Mode} {Seeking}},
	isbn = {978-3-540-88693-8},
	doi = {10.1007/978-3-540-88693-8_52},
	abstract = {We show that the complexity of the recently introduced medoid-shift algorithm in clustering N points is O(N2), with a small constant, if the underlying distance is Euclidean. This makes medoid shift considerably faster than mean shift, contrarily to what previously believed. We then exploit kernel methods to extend both mean shift and the improved medoid shift to a large family of distances, with complexity bounded by the effective rank of the resulting kernel matrix, and with explicit regularization constraints. Finally, we show that, under certain conditions, medoid shift fails to cluster data points belonging to the same mode, resulting in over-fragmentation. We propose remedies for this problem, by introducing a novel, simple and extremely efficient clustering algorithm, called quick shift, that explicitly trades off under- and over-fragmentation. Like medoid shift, quick shift operates in non-Euclidean spaces in a straightforward manner. We also show that the accelerated medoid shift can be used to initialize mean shift for increased efficiency. We illustrate our algorithms to clustering data on manifolds, image segmentation, and the automatic discovery of visual categories.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2008},
	publisher = {Springer},
	author = {Vedaldi, Andrea and Soatto, Stefano},
	editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
	year = {2008},
	keywords = {Gaussian Window, Image Segmentation, Kernel Matrix, Kernel Method, Kernel Space},
	pages = {705--718},
}

@article{achanta_slic_2012,
	title = {{SLIC} {Superpixels} {Compared} to {State}-of-the-{Art} {Superpixel} {Methods}},
	volume = {34},
	doi = {10.1109/TPAMI.2012.120},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurélien and Fua, Pascal and Susstrunk, Sabine},
	month = may,
	year = {2012},
}

@article{lang_object_2010,
	title = {Object validity for operational tasks in a policy context},
	volume = {55},
	issn = {1449-8596, 1836-5655},
	url = {http://www.tandfonline.com/doi/abs/10.1080/14498596.2010.487639},
	doi = {10.1080/14498596.2010.487639},
	language = {en},
	number = {1},
	urldate = {2024-01-08},
	journal = {Journal of Spatial Science},
	author = {Lang, S. and Albrecht, F. and Kienberger, S. and Tiede, D.},
	month = jun,
	year = {2010},
	pages = {9--22},
}

@article{lang_hierarchical_2003,
	title = {Hierarchical {Object} {Representation} – {Comparative} {Multi}-{Scale} {Mapping} of {Anthropogenic} and {Natural} {Features}},
	abstract = {Hierarchical feature representation through multi-scale segmentation offers new possibilities in mapping complex systems. We lay out that the recognition of natural features is more difficult than the recognition of anthropogenic features such as houses or roads. For the latter group spectral and spatial characteristics can be anticipated and rules can be defined. Consequently, the automated extraction of roads or the 3D extraction of buildings is very advanced. In contrast, the automatic extraction of natural features like habitats is still far from being operational. We discuss reasons for that and highlight an object-based image segmentation methodology which incorporates spectral, spatial, topological and hierarchical characteristics of the objects within a semantic network.},
	language = {en},
	author = {Lang, S and Blaschke, T},
	year = {2003},
}

@article{burnett_multi-scale_2003,
	title = {A multi-scale segmentation/object relationship modelling methodology for landscape analysis},
	volume = {168},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S030438000300139X},
	doi = {10.1016/S0304-3800(03)00139-X},
	abstract = {Natural complexity can best be explored using spatial analysis tools based on concepts of landscape as process continuums that can be partially decomposed into objects or patches. We introduce a ﬁve-step methodology based on multi-scale segmentation and object relationship modelling. Hierarchical patch dynamics (HPD) is adopted as the theoretical framework to address issues of heterogeneity, scale, connectivity and quasi-equilibriums in landscapes. Remote sensing has emerged as the most useful data source for characterizing land use/land cover but a vast majority of applications rely on basic image processing concepts developed in the 1970s: one spatial scale, per-pixel classiﬁcation of a multi-scale spectral feature space. We argue that this methodology does not make sufﬁcient use of spatial concepts of neighbourhood, proximity or homogeneity. In contrast, the authors demonstrate in this article the utility of the HPD framework as a theoretical basis for landscape analysis in two different projects using alternative image processing methodologies, which try to overcome the ‘pixel-centred’ view.},
	language = {en},
	number = {3},
	urldate = {2024-01-08},
	journal = {Ecological Modelling},
	author = {Burnett, C and Blaschke, Thomas},
	month = oct,
	year = {2003},
	pages = {233--249},
}

@article{levine_rule-based_1985,
	title = {Rule-based image segmentation: {A} dynamic control strategy approach},
	volume = {32},
	issn = {0734-189X},
	shorttitle = {Rule-based image segmentation},
	url = {https://www.sciencedirect.com/science/article/pii/0734189X85900040},
	doi = {10.1016/0734-189X(85)90004-0},
	abstract = {The structure and functional aspects of a rule-based image segmentation system are briefly described. This paper focuses mainly on the control aspects of the system. A set of measurements is defined that evaluates the quality of an image segmentation and thereby determines the control. To accomplish this, focus of attention areas are created within the image. These are processed in an order that depends on their properties, as reflected by the set of performance measures, computed for each area. A dynamic control strategy within each area is also determined based on the individual characteristics of that area. One aspect involves determining the spatial order in which the system will process the data entries inside the area. In addition, an ordering must be established among all the rules included in the model. Dynamic strategy setting is formulated as a fuzzy decision-making problem, whose solution depends on the performance parameters. Because the individual characteristics of areas are reflected in different performance measurements, the resulting control strategies will vary from one area to the next. In addition, the strategy within each area will vary with time to reflect the changing properties. This spatial and temporal updating process is designed to ensure both efficiency and improved output.},
	number = {1},
	urldate = {2024-01-03},
	journal = {Computer Vision, Graphics, and Image Processing},
	author = {Levine, Martin D. and Nazif, Ahmed M.},
	month = oct,
	year = {1985},
	pages = {104--126},
}

@article{ebert_urban_2009,
	title = {Urban social vulnerability assessment with physical proxies and spatial metrics derived from air- and spaceborne imagery and {GIS} data},
	volume = {48},
	issn = {1573-0840},
	url = {https://doi.org/10.1007/s11069-008-9264-0},
	doi = {10.1007/s11069-008-9264-0},
	abstract = {Risk management in urban planning is of increasing importance to mitigate the growing amount of damage and the increasing number of casualties caused by natural disasters. Risk assessment to support management requires knowledge about present and future hazards, elements at risk and different types of vulnerability. This article deals with the assessment of social vulnerability (SV). In the past this has frequently been neglected due to lack of data and assessment difficulties. Existing approaches for SV assessment, primarily based on community-based methods or on census data, have limited efficiency and transferability. In this article a new method based on contextual analysis of image and GIS data is presented. An approach based on proxy variables that were derived from high-resolution optical and laser scanning data was applied, in combination with elevation information and existing hazard data. Object-oriented image analysis was applied for the definition and estimation of those variables, focusing on SV indicators with physical characteristics. A reference Social Vulnerability Index (SVI) was created from census data available for the study area on a neighbourhood level and tested for parts of Tegucigalpa, Honduras. For the evaluation of the proxy-variables, a stepwise regression model to select the best explanatory variables for changes in the SVI was applied. Eight out of 47 variables explained almost 60\% of the variance, whereby the slope position and the proportion of built-up area in a neighbourhood were found to be the most valuable proxies. This work shows that contextual segmentation-based analysis of geospatial data can substantially aid in SV assessment and, when combined with field-based information, leads to optimization in terms of assessment frequency and cost.},
	language = {en},
	number = {2},
	urldate = {2024-01-03},
	journal = {Natural Hazards},
	author = {Ebert, Annemarie and Kerle, Norman and Stein, Alfred},
	month = feb,
	year = {2009},
	keywords = {Object-oriented image analysis, Proxy variables, Social vulnerability assessment, Tegucigalpa},
	pages = {275--294},
}

@incollection{kressler_object-oriented_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Geoinformation} and {Cartography}},
	title = {Object-oriented analysis of image and {LiDAR} data and its potential for a dasymetric mapping application},
	isbn = {978-3-540-77058-9},
	url = {https://doi.org/10.1007/978-3-540-77058-9_33},
	abstract = {In this chapter, object-oriented image analysis, on the basis of eCognition, is used to classify an IKONOS-2 image together with data derived from an aerial laser scanner. After the initialclassification of different land cover types, a subset containing only buildings was integrated with zoning as well as population data. Zoning data was used to assign the classified buildings to different land use types, thus allowing the identification of buildings designated for residential use. On these were mapped population data, available on a 250 m grid basis while taking into account the height of each building, allowing a differentiated presentation of where people live within in each grid cell. Comparison with reference data suggest a high potential not only for further dasymetric mapping applications, but other applications as well.},
	language = {en},
	urldate = {2024-01-03},
	booktitle = {Object-{Based} {Image} {Analysis}: {Spatial} {Concepts} for {Knowledge}-{Driven} {Remote} {Sensing} {Applications}},
	publisher = {Springer},
	author = {Kressler, F. and Steinnocher, K.},
	editor = {Blaschke, Thomas and Lang, Stefan and Hay, Geoffrey J.},
	year = {2008},
	doi = {10.1007/978-3-540-77058-9_33},
	keywords = {building height, census data, population disaggregation, urban land use},
	pages = {611--624},
}

@article{zhou_objectoriented_2008,
	title = {An object‐oriented approach for analysing and characterizing urban landscape at the parcel level},
	volume = {29},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431160701469065},
	doi = {10.1080/01431160701469065},
	abstract = {This paper presents an object‐oriented approach for analysing and characterizing the urban landscape structure at the parcel level using high‐resolution digital aerial imagery and LIght Detection and Ranging (LIDAR) data. Additional spatial datasets including property parcel boundaries and building footprints were used to both facilitate object segmentation and obtain greater classification accuracy. The study area is the Gwynns Falls watershed, which includes portions of Baltimore City and Baltimore County, MD. A three‐level hierarchical network of image objects was generated, and objects were classified. At the two lower levels, objects were classified into five classes, building, pavement, bare soil, fine textured vegetation and coarse textured vegetation, respectively. The object‐oriented classification approach proved to be effective for urban land cover classification. The overall accuracy of the classification was 92.3\%, and the overall Kappa statistic was 0.899. Land cover proportions as well as vegetation characteristics were then summarized by property parcel. This exercise resulted in a knowledge base of rules for urban land cover classification, which could potentially be applied to other urban areas.},
	number = {11},
	urldate = {2024-01-03},
	journal = {International Journal of Remote Sensing},
	author = {Zhou, W. and Troy, A.},
	month = jun,
	year = {2008},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01431160701469065},
	pages = {3119--3135},
}

@article{chubey_object-based_2006,
	title = {Object-based {Analysis} of {Ikonos}-2 {Imagery} for {Extraction} of {Forest} {Inventory} {Parameters}},
	volume = {72},
	doi = {10.14358/PERS.72.4.383},
	abstract = {A method is presented for deriving forest inventory information from Ikonos-2 imagery based on the analysis of image objects rather than more conventional pixel-based image analysis approaches. For a 77 km2 study area in southwestern Alberta, Canada, image objects representing
homogeneous landscape components were delineated from Ikonos-2 data using an image segmentation routine. Decision tree statistical analyses were used to identify correlations between metrics derived from spectral and spatial properties of the image objects and field-derived samples of individual
forest inventory parameters. The strongest relationships were observed for classes of discrete land-cover types, species composition, and crown closure.},
	number = {4},
	journal = {Photogrammetric Engineering \& Remote Sensing},
	author = {Chubey, Michael S. and Franklin, Steven E. and Wulder, Michael A.},
	month = apr,
	year = {2006},
	pages = {383--394},
}

@article{stutz_superpixels_2018-1,
	title = {Superpixels: {An} evaluation of the state-of-the-art},
	volume = {166},
	issn = {1077-3142},
	shorttitle = {Superpixels},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314217300589},
	doi = {10.1016/j.cviu.2017.03.007},
	abstract = {Superpixels group perceptually similar pixels to create visually meaningful entities while heavily reducing the number of primitives for subsequent processing steps. As of these properties, superpixel algorithms have received much attention since their naming in 2003 (Ren and Malik, 2003). By today, publicly available superpixel algorithms have turned into standard tools in low-level vision. As such, and due to their quick adoption in a wide range of applications, appropriate benchmarks are crucial for algorithm selection and comparison. Until now, the rapidly growing number of algorithms as well as varying experimental setups hindered the development of a unifying benchmark. We present a comprehensive evaluation of 28 state-of-the-art superpixel algorithms utilizing a benchmark focussing on fair comparison and designed to provide new insights relevant for applications. To this end, we explicitly discuss parameter optimization and the importance of strictly enforcing connectivity. Furthermore, by extending well-known metrics, we are able to summarize algorithm performance independent of the number of generated superpixels, thereby overcoming a major limitation of available benchmarks. Furthermore, we discuss runtime, robustness against noise, blur and affine transformations, implementation details as well as aspects of visual quality. Finally, we present an overall ranking of superpixel algorithms which redefines the state-of-the-art and enables researchers to easily select appropriate algorithms and the corresponding implementations which themselves are made publicly available as part of our benchmark at http://www.davidstutz.de/projects/superpixel-benchmark/.},
	language = {en},
	urldate = {2023-06-12},
	journal = {Computer Vision and Image Understanding},
	author = {Stutz, David and Hermans, Alexander and Leibe, Bastian},
	month = jan,
	year = {2018},
	keywords = {Benchmark, Evaluation, Image segmentation, Perceptual grouping, Superpixel segmentation, Superpixels},
	pages = {1--27},
}

@inproceedings{lagrange_benchmarking_2015,
	title = {Benchmarking classification of earth-observation data: {From} learning explicit features to convolutional networks},
	shorttitle = {Benchmarking classification of earth-observation data},
	url = {https://ieeexplore.ieee.org/document/7326745?denied=},
	doi = {10.1109/IGARSS.2015.7326745},
	abstract = {In this paper, we address the task of semantic labeling of multisource earth-observation (EO) data. Precisely, we benchmark several concurrent methods of the last 15 years, from expert classifiers, spectral support-vector classification and high-level features to deep neural networks. We establish that (1) combining multisensor features is essential for retrieving some specific classes, (2) in the image domain, deep convolutional networks obtain significantly better overall performances and (3) transfer of learning from large generic-purpose image sets is highly effective to build EO data classifiers.},
	urldate = {2023-12-10},
	booktitle = {2015 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium} ({IGARSS})},
	author = {Lagrange, Adrien and Le Saux, Bertrand and Beaupère, Anne and Boulch, Alexandre and Chan-Hon-Tong, Adrien and Herbin, Stéphane and Randrianarivo, Hicham and Ferecatu, Marin},
	month = jul,
	year = {2015},
	note = {ISSN: 2153-7003},
	pages = {4173--4176},
}

@article{lagrange_benchmarking_2015-1,
	title = {Benchmarking classification of earth-observation data: {From} learning explicit features to convolutional networks},
	shorttitle = {Benchmarking classification of earth-observation data},
	url = {http://ieeexplore.ieee.org/document/7326745/},
	doi = {10.1109/IGARSS.2015.7326745},
	abstract = {In this paper, we address the task of semantic labeling of multisource earth-observation (EO) data. Precisely, we benchmark several concurrent methods of the last 15 years, from expert classifiers, spectral support-vector classification and high-level features to deep neural networks. We establish that (1) combining multisensor features is essential for retrieving some specific classes, (2) in the image domain, deep convolutional networks obtain significantly better overall performances and (3) transfer of learning from large generic-purpose image sets is highly effective to build EO data classifiers.},
	urldate = {2023-12-10},
	journal = {2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
	author = {Lagrange, Adrien and Le Saux, Bertrand and Beaupere, Anne and Boulch, Alexandre and Chan-Hon-Tong, Adrien and Herbin, Stephane and Randrianarivo, Hicham and Ferecatu, Marin},
	month = jul,
	year = {2015},
	note = {Conference Name: IGARSS 2015 - 2015 IEEE International Geoscience and Remote Sensing Symposium
ISBN: 9781479979295
Place: Milan, Italy
Publisher: IEEE},
	pages = {4173--4176},
}

@article{interdonato_feature-rich_2019,
	title = {Feature-rich networks: going beyond complex network topologies},
	volume = {4},
	copyright = {2019 The Author(s)},
	issn = {2364-8228},
	shorttitle = {Feature-rich networks},
	url = {https://appliednetsci.springeropen.com/articles/10.1007/s41109-019-0111-x},
	doi = {10.1007/s41109-019-0111-x},
	abstract = {The growing availability of multirelational data gives rise to an opportunity for novel characterization of complex real-world relations, supporting the proliferation of diverse network models such as Attributed Graphs, Heterogeneous Networks, Multilayer Networks, Temporal Networks, Location-aware Networks, Knowledge Networks, Probabilistic Networks, and many other task-driven and data-driven models. In this paper, we propose an overview of these models and their main applications, described under the common denomination of Feature-rich Networks, i. e. models where the expressive power of the network topology is enhanced by exposing one or more peculiar features. The aim is also to sketch a scenario that can inspire the design of novel feature-rich network models, which in turn can support innovative methods able to exploit the full potential of mining complex network structures in domain-specific applications.},
	language = {en},
	number = {1},
	urldate = {2023-11-22},
	journal = {Applied Network Science},
	author = {Interdonato, Roberto and Atzmueller, Martin and Gaito, Sabrina and Kanawati, Rushed and Largeron, Christine and Sala, Alessandra},
	month = dec,
	year = {2019},
	note = {Number: 1
Publisher: SpringerOpen},
	pages = {1--13},
}

@misc{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	doi = {10.48550/arXiv.1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2023-11-16},
	publisher = {arXiv},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = oct,
	year = {2018},
	note = {arXiv:1806.01261 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{fleet_visualizing_2014,
	address = {Cham},
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	volume = {8689},
	isbn = {978-3-319-10589-5 978-3-319-10590-1},
	url = {http://link.springer.com/10.1007/978-3-319-10590-1_53},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classiﬁcation performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classiﬁer. Used in a diagnostic role, these visualizations allow us to ﬁnd model architectures that outperform Krizhevsky et al. on the ImageNet classiﬁcation benchmark. We also perform an ablation study to discover the performance contribution from diﬀerent model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classiﬁer is retrained, it convincingly beats the current state-ofthe-art results on Caltech-101 and Caltech-256 datasets.},
	language = {en},
	urldate = {2023-11-16},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	doi = {10.1007/978-3-319-10590-1_53},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {818--833},
}

@article{hamilton_graph_nodate,
	title = {Graph {Representation} {Learning}},
	abstract = {Graph-structured data is ubiquitous throughout the natural and social sciences, from telecommunication networks to quantum chemistry. Building relational inductive biases into deep learning architectures is crucial if we want systems that can learn, reason, and generalize from this kind of data. Recent years have seen a surge in research on graph representation learning, including techniques for deep graph embeddings, generalizations of convolutional neural networks to graph-structured data, and neural message-passing approaches inspired by belief propagation. These advances in graph representation learning have led to new state-of-the-art results in numerous domains, including chemical synthesis, 3D-vision, recommender systems, question answering, and social network analysis.},
	language = {en},
	author = {Hamilton, William L},
}

@article{georgousis_graph_2021,
	title = {Graph {Deep} {Learning}: {State} of the {Art} and {Challenges}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Graph {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/abstract/document/9339909},
	doi = {10.1109/ACCESS.2021.3055280},
	abstract = {The last half-decade has seen a surge in deep learning research on irregular domains and efforts to extend convolutional neural networks (CNNs) to work on irregularly structured data. The graph has emerged as a particularly useful geometrical object in deep learning, able to represent a variety of irregular domains well. Graphs can represent various complex systems, from molecular structure, to computer and social and traffic networks. Consequent on the extension of CNNs to graphs, a great amount of research has been published that improves the inferential power and computational efficiency of graph-based convolutional neural networks (GCNNs). The research is incipient, however, and our understanding is relatively rudimentary. The majority of GCNNs are designed to operate with certain properties. In this survey we review of the state of graph representation learning from the perspective of deep learning. We consider challenges in graph deep learning that have been neglected in the majority of work, largely because of the numerous theoretical difficulties they present. We identify four major challenges in graph deep learning: dynamic and evolving graphs, learning with edge signals and information, graph estimation, and the generalization of graph models. For each problem we discuss the theoretical and practical issues, survey the relevant research, while highlighting the limitations of the state of the art. Advances on these challenges would permit GCNNs to be applied to wider range of domains, in situations where graph models have previously been limited owing to the obstructions to applying a model owing to the domains' natures.},
	urldate = {2023-11-15},
	journal = {IEEE Access},
	author = {Georgousis, Stavros and Kenning, Michael P. and Xie, Xianghua},
	year = {2021},
	note = {Conference Name: IEEE Access},
	pages = {22106--22140},
}

@article{wang_dynamic_2019,
	title = {Dynamic {Graph} {CNN} for {Learning} on {Point} {Clouds}},
	volume = {38},
	issn = {0730-0301},
	url = {https://dl.acm.org/doi/10.1145/3326362},
	doi = {10.1145/3326362},
	abstract = {Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information, so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds, including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks, including ModelNet40, ShapeNetPart, and S3DIS.},
	number = {5},
	urldate = {2023-06-15},
	journal = {ACM Transactions on Graphics},
	author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
	month = oct,
	year = {2019},
	keywords = {Point cloud, classification, segmentation},
	pages = {146:1--146:12},
}

@misc{chen_survey_2022,
	title = {A {Survey} on {Graph} {Neural} {Networks} and {Graph} {Transformers} in {Computer} {Vision}: {A} {Task}-{Oriented} {Perspective}},
	shorttitle = {A {Survey} on {Graph} {Neural} {Networks} and {Graph} {Transformers} in {Computer} {Vision}},
	url = {http://arxiv.org/abs/2209.13232},
	doi = {10.48550/arXiv.2209.13232},
	abstract = {Graph Neural Networks (GNNs) have gained momentum in graph representation learning and boosted the state of the art in a variety of areas, such as data mining ({\textbackslash}emph\{e.g.,\} social network analysis and recommender systems), computer vision ({\textbackslash}emph\{e.g.,\} object detection and point cloud learning), and natural language processing ({\textbackslash}emph\{e.g.,\} relation extraction and sequence learning), to name a few. With the emergence of Transformers in natural language processing and computer vision, graph Transformers embed a graph structure into the Transformer architecture to overcome the limitations of local neighborhood aggregation while avoiding strict structural inductive biases. In this paper, we present a comprehensive review of GNNs and graph Transformers in computer vision from a task-oriented perspective. Specifically, we divide their applications in computer vision into five categories according to the modality of input data, {\textbackslash}emph\{i.e.,\} 2D natural images, videos, 3D data, vision + language, and medical images. In each category, we further divide the applications according to a set of vision tasks. Such a task-oriented taxonomy allows us to examine how each task is tackled by different GNN-based approaches and how well these approaches perform. Based on the necessary preliminaries, we provide the definitions and challenges of the tasks, in-depth coverage of the representative approaches, as well as discussions regarding insights, limitations, and future directions.},
	urldate = {2022-11-10},
	publisher = {arXiv},
	author = {Chen, Chaoqi and Wu, Yushuang and Dai, Qiyuan and Zhou, Hong-Yu and Xu, Mutian and Yang, Sibei and Han, Xiaoguang and Yu, Yizhou},
	month = oct,
	year = {2022},
	note = {arXiv:2209.13232 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{velickovic_everything_2023-1,
	title = {Everything is connected: {Graph} neural networks},
	volume = {79},
	issn = {0959-440X},
	shorttitle = {Everything is connected},
	url = {https://www.sciencedirect.com/science/article/pii/S0959440X2300012X},
	doi = {10.1016/j.sbi.2023.102538},
	abstract = {In many ways, graphs are the main modality of data we receive from nature. This is due to the fact that most of the patterns we see, both in natural and artificial systems, are elegantly representable using the language of graph structures. Prominent examples include molecules (represented as graphs of atoms and bonds), social networks and transportation networks. This potential has already been seen by key scientific and industrial groups, with already-impacted application areas including traffic forecasting, drug discovery, social network analysis and recommender systems. Further, some of the most successful domains of application for machine learning in previous years—images, text and speech processing—can be seen as special cases of graph representation learning, and consequently there has been significant exchange of information between these areas. The main aim of this short survey is to enable the reader to assimilate the key concepts in the area, and position graph representation learning in a proper context with related fields.},
	language = {en},
	urldate = {2023-06-12},
	journal = {Current Opinion in Structural Biology},
	author = {Veličković, Petar},
	month = apr,
	year = {2023},
	keywords = {Drug discovery, GNNs, Graph neural networks, Graph representation learning, Traffic forecasting, Transformers},
	pages = {102538},
}

@misc{danel_spatial_2020-1,
	title = {Spatial {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1909.05310},
	doi = {10.48550/arXiv.1909.05310},
	abstract = {Graph Convolutional Networks (GCNs) have recently become the primary choice for learning from graph-structured data, superseding hash fingerprints in representing chemical compounds. However, GCNs lack the ability to take into account the ordering of node neighbors, even when there is a geometric interpretation of the graph vertices that provides an order based on their spatial positions. To remedy this issue, we propose Spatial Graph Convolutional Network (SGCN) which uses spatial features to efficiently learn from graphs that can be naturally located in space. Our contribution is threefold: we propose a GCN-inspired architecture which (i) leverages node positions, (ii) is a proper generalization of both GCNs and Convolutional Neural Networks (CNNs), (iii) benefits from augmentation which further improves the performance and assures invariance with respect to the desired properties. Empirically, SGCN outperforms state-of-the-art graph-based methods on image classification and chemical tasks.},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Danel, Tomasz and Spurek, Przemysław and Tabor, Jacek and Śmieja, Marek and Struski, Łukasz and Słowik, Agnieszka and Maziarka, Łukasz},
	month = jul,
	year = {2020},
	note = {arXiv:1909.05310 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{schmitt_there_2023,
	title = {There {Are} {No} {Data} {Like} {More} {Data}: {Datasets} for deep learning in {Earth} observation},
	volume = {11},
	issn = {2168-6831},
	shorttitle = {There {Are} {No} {Data} {Like} {More} {Data}},
	url = {https://ieeexplore.ieee.org/abstract/document/10213439?casa_token=tNOS_kAeSPgAAAAA:Z3qohKjf-Vs9-iChNZGDhAvorC3MVjPlg1RibcAvGeUZejwxxlTrh6zTuO3osuPVBa0UHWHLMfrk},
	doi = {10.1109/MGRS.2023.3293459},
	abstract = {Carefully curated and annotated datasets are the foundation of machine learning (ML), with particularly data-hungry deep neural networks forming the core of what is often called artificial intelligence (AI). Due to the massive success of deep learning (DL) applied to Earth observation (EO) problems, the focus of the community has been largely on the development of evermore sophisticated deep neural network architectures and training strategies. For that purpose, numerous task-specific datasets have been created that were largely ignored by previously published review articles on AI for EO. With this article, we want to change the perspective and put ML datasets dedicated to EO data and applications into the spotlight. Based on a review of historical developments, currently available resources are described and a perspective for future developments is formed. We hope to contribute to an understanding that the nature of our data is what distinguishes the EO community from many other communities that apply DL techniques to image data, and that a detailed understanding of EO data peculiarities is among the core competencies of our discipline.},
	number = {3},
	urldate = {2023-10-09},
	journal = {IEEE Geoscience and Remote Sensing Magazine},
	author = {Schmitt, Michael and Ahmadi, Seyed Ali and Xu, Yonghao and Taşkin, Gülşen and Verma, Ujjwal and Sica, Francescopaolo and Hänsch, Ronny},
	month = sep,
	year = {2023},
	note = {Conference Name: IEEE Geoscience and Remote Sensing Magazine},
	pages = {63--97},
}

@article{bronstein_geometric_2017,
	title = {Geometric {Deep} {Learning}: {Going} beyond {Euclidean} data},
	volume = {34},
	issn = {1558-0792},
	shorttitle = {Geometric {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/document/7974879},
	doi = {10.1109/MSP.2017.2693418},
	abstract = {Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains, such as graphs and manifolds. The purpose of this article is to overview different examples of geometric deep-learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
	number = {4},
	urldate = {2023-11-15},
	journal = {IEEE Signal Processing Magazine},
	author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	month = jul,
	year = {2017},
	note = {Conference Name: IEEE Signal Processing Magazine},
	pages = {18--42},
}

@misc{noauthor_dstl_nodate,
	title = {Dstl {Satellite} {Imagery} {Feature} {Detection}},
	url = {https://kaggle.com/competitions/dstl-satellite-imagery-feature-detection},
	abstract = {Can you train an eye in the sky?},
	language = {en},
	urldate = {2023-11-15},
}

@inproceedings{boguszewski_landcoverai_2021,
	title = {{LandCover}.ai: {Dataset} for {Automatic} {Mapping} of {Buildings}, {Woodlands}, {Water} and {Roads} {From} {Aerial} {Imagery}},
	shorttitle = {{LandCover}.ai},
	url = {https://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Boguszewski_LandCover.ai_Dataset_for_Automatic_Mapping_of_Buildings_Woodlands_Water_and_CVPRW_2021_paper.html},
	language = {en},
	urldate = {2023-11-15},
	author = {Boguszewski, Adrian and Batorski, Dominik and Ziemba-Jankowska, Natalia and Dziedzic, Tomasz and Zambrzycka, Anna},
	year = {2021},
	pages = {1102--1110},
}

@article{rottensteiner_isprs_2012-1,
	title = {The {ISPRS} benchmark on urban object classification and 3d building reconstruction},
	volume = {I-3},
	copyright = {CC BY 3.0 Unported},
	url = {https://www.repo.uni-hannover.de/handle/123456789/5086},
	doi = {10.15488/5042},
	abstract = {For more than two decades, many efforts have been made to develop methods for extracting urban objects from data acquired by airborne sensors. In order to make the results of such algorithms more comparable, benchmarking data sets are of paramount importance. Such a data set, consisting of airborne image and laserscanner data, has been made available to the scientific community. Researchers were encouraged to submit results of urban object detection and 3D building reconstruction, which were evaluated based on reference data. This paper presents the outcomes of the evaluation for building detection, tree detection, and 3D building reconstruction. The results achieved by different methods are compared and analysed to identify promising strategies for automatic urban object extraction from current airborne sensor data, but also common problems of state-of-the-art methods.},
	language = {eng},
	number = {1},
	urldate = {2023-11-15},
	journal = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences ; I-3},
	author = {Rottensteiner, Franz and Sohn, Gunho and Jung, Jaewook and Gerke, Markus and Baillard, Caroline and Benitez, Sebastien and Breitkopf, Uwe},
	year = {2012},
	note = {Publisher: Göttingen : Copernicus GmbH},
	pages = {293--298},
}

@article{blaschke_object_2010,
	title = {Object based image analysis for remote sensing},
	volume = {65},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271609000884},
	doi = {10.1016/j.isprsjprs.2009.06.004},
	abstract = {Remote sensing imagery needs to be converted into tangible information which can be utilised in conjunction with other data sets, often within widely used Geographic Information Systems (GIS). As long as pixel sizes remained typically coarser than, or at the best, similar in size to the objects of interest, emphasis was placed on per-pixel analysis, or even sub-pixel analysis for this conversion, but with increasing spatial resolutions alternative paths have been followed, aimed at deriving objects that are made up of several pixels. This paper gives an overview of the development of object based methods, which aim to delineate readily usable objects from imagery while at the same time combining image processing and GIS functionalities in order to utilize spectral and contextual information in an integrative way. The most common approach used for building objects is image segmentation, which dates back to the 1970s. Around the year 2000 GIS and image processing started to grow together rapidly through object based image analysis (OBIA - or GEOBIA for geospatial object based image analysis). In contrast to typical Landsat resolutions, high resolution images support several scales within their images. Through a comprehensive literature review several thousand abstracts have been screened, and more than 820 OBIA-related articles comprising 145 journal papers, 84 book chapters and nearly 600 conference papers, are analysed in detail. It becomes evident that the first years of the OBIA/GEOBIA developments were characterised by the dominance of ‘grey’ literature, but that the number of peer-reviewed journal articles has increased sharply over the last four to five years. The pixel paradigm is beginning to show cracks and the OBIA methods are making considerable progress towards a spatially explicit information extraction workflow, such as is required for spatial planning as well as for many monitoring programmes.},
	language = {en},
	number = {1},
	urldate = {2023-04-18},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Blaschke, T.},
	month = jan,
	year = {2010},
	keywords = {GEOBIA, GIScience, Multiscale image analysis, OBIA, Object based image analysis},
	pages = {2--16},
}

@article{kavran_graph_2023,
	title = {Graph {Neural} {Network}-{Based} {Method} of {Spatiotemporal} {Land} {Cover} {Mapping} {Using} {Satellite} {Imagery}},
	volume = {23},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/23/14/6648},
	doi = {10.3390/s23146648},
	abstract = {Multispectral satellite imagery offers a new perspective for spatial modelling, change detection and land cover classification. The increased demand for accurate classification of geographically diverse regions led to advances in object-based methods. A novel spatiotemporal method is presented for object-based land cover classification of satellite imagery using a Graph Neural Network. This paper introduces innovative representation of sequential satellite images as a directed graph by connecting segmented land region through time. The method’s novel modular node classification pipeline utilises the Convolutional Neural Network as a multispectral image feature extraction network, and the Graph Neural Network as a node classification model. To evaluate the performance of the proposed method, we utilised EfficientNetV2-S for feature extraction and the GraphSAGE algorithm with Long Short-Term Memory aggregation for node classification. This innovative application on Sentinel-2 L2A imagery produced complete 4-year intermonthly land cover classification maps for two regions: Graz in Austria, and the region of Portorož, Izola and Koper in Slovenia. The regions were classified with Corine Land Cover classes. In the level 2 classification of the Graz region, the method outperformed the state-of-the-art UNet model, achieving an average F1-score of 0.841 and an accuracy of 0.831, as opposed to UNet’s 0.824 and 0.818, respectively. Similarly, the method demonstrated superior performance over UNet in both regions under the level 1 classification, which contains fewer classes. Individual classes have been classified with accuracies up to 99.17\%.},
	language = {en},
	number = {14},
	urldate = {2023-08-28},
	journal = {Sensors},
	author = {Kavran, Domen and Mongus, Domen and Žalik, Borut and Lukač, Niko},
	month = jan,
	year = {2023},
	note = {Number: 14
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {EfficientNetV2, GraphSAGE, Sentinel-2, multispectral, node, superpixel},
	pages = {6648},
}

@misc{ahmad_learning_2023,
	title = {Learning adjacency matrix for dynamic graph neural network},
	url = {http://arxiv.org/abs/2310.02606},
	doi = {10.48550/arXiv.2310.02606},
	abstract = {In recent work, [1] introduced the concept of using a Block Adjacency Matrix (BA) for the representation of spatio-temporal data. While their method successfully concatenated adjacency matrices to encapsulate spatio-temporal relationships in a single graph, it formed a disconnected graph. This limitation hampered the ability of Graph Convolutional Networks (GCNs) to perform message passing across nodes belonging to different time steps, as no temporal links were present. To overcome this challenge, we introduce an encoder block specifically designed to learn these missing temporal links. The encoder block processes the BA and predicts connections between previously unconnected subgraphs, resulting in a Spatio-Temporal Block Adjacency Matrix (STBAM). This enriched matrix is then fed into a Graph Neural Network (GNN) to capture the complex spatio-temporal topology of the network. Our evaluations on benchmark datasets, surgVisDom and C2D2, demonstrate that our method, with slightly higher complexity, achieves superior results compared to state-of-the-art results. Our approach's computational overhead remains significantly lower than conventional non-graph-based methodologies for spatio-temporal data.},
	urldate = {2023-10-24},
	publisher = {arXiv},
	author = {Ahmad, Osama and Jalil, Omer Abdul and Nazir, Usman and Taj, Murtaza},
	month = oct,
	year = {2023},
	note = {arXiv:2310.02606 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{ma_attention_2019,
	title = {Attention {Graph} {Convolution} {Network} for {Image} {Segmentation} in {Big} {SAR} {Imagery} {Data}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/11/21/2586},
	doi = {10.3390/rs11212586},
	abstract = {The recent emergence of high-resolution Synthetic Aperture Radar (SAR) images leads to massive amounts of data. In order to segment these big remotely sensed data in an acceptable time frame, more and more segmentation algorithms based on deep learning attempt to take superpixels as processing units. However, the over-segmented images become non-Euclidean structure data that traditional deep Convolutional Neural Networks (CNN) cannot directly process. Here, we propose a novel Attention Graph Convolution Network (AGCN) to perform superpixel-wise segmentation in big SAR imagery data. AGCN consists of an attention mechanism layer and Graph Convolution Networks (GCN). GCN can operate on graph-structure data by generalizing convolutions to the graph domain and have been successfully applied in tasks such as node classification. The attention mechanism layer is introduced to guide the graph convolution layers to focus on the most relevant nodes in order to make decisions by specifying different coefficients to different nodes in a neighbourhood. The attention layer is located before the convolution layers, and noisy information from the neighbouring nodes has less negative influence on the attention coefficients. Quantified experiments on two airborne SAR image datasets prove that the proposed method outperforms the other state-of-the-art segmentation approaches. Its computation time is also far less than the current mainstream pixel-level semantic segmentation networks.},
	language = {en},
	number = {21},
	urldate = {2023-04-14},
	journal = {Remote Sensing},
	author = {Ma, Fei and Gao, Fei and Sun, Jinping and Zhou, Huiyu and Hussain, Amir},
	month = jan,
	year = {2019},
	note = {Number: 21
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Graph Convolution Network (GCN), Synthetic Aperture Radar (SAR), attention mechanism, big data, neighbourhood consistency, segmentation},
	pages = {2586},
}

@inproceedings{garnot_time-space_2019,
	address = {Yokohama, Japan},
	title = {Time-{Space} {Tradeoff} in {Deep} {Learning} {Models} for {Crop} {Classification} on {Satellite} {Multi}-{Spectral} {Image} {Time} {Series}},
	isbn = {978-1-5386-9154-0},
	url = {https://ieeexplore.ieee.org/document/8900517/},
	doi = {10.1109/IGARSS.2019.8900517},
	language = {en},
	urldate = {2023-11-14},
	booktitle = {{IGARSS} 2019 - 2019 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	publisher = {IEEE},
	author = {Garnot, V. Sainte Fare and Landrieu, L. and Giordano, S. and Chehata, N.},
	month = jul,
	year = {2019},
	pages = {6247--6250},
}

@inproceedings{garnot_satellite_2020,
	title = {Satellite {Image} {Time} {Series} {Classification} {With} {Pixel}-{Set} {Encoders} and {Temporal} {Self}-{Attention}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Garnot_Satellite_Image_Time_Series_Classification_With_Pixel-Set_Encoders_and_Temporal_CVPR_2020_paper.html},
	urldate = {2023-06-09},
	author = {Garnot, Vivien Sainte Fare and Landrieu, Loic and Giordano, Sebastien and Chehata, Nesrine},
	year = {2020},
	pages = {12325--12334},
}

@article{zhao_superpixel_2022,
	title = {Superpixel {Guided} {Deformable} {Convolution} {Network} for {Hyperspectral} {Image} {Classification}},
	volume = {31},
	issn = {1057-7149, 1941-0042},
	url = {https://ieeexplore.ieee.org/document/9782104/},
	doi = {10.1109/TIP.2022.3176537},
	abstract = {Convolutional neural networks are widely used in the ﬁeld of hyperspectral image classiﬁcation because of their excellent nonlinear feature extraction ability. However, as the sampling position of the regular convolution kernel is unchangeable, the regular convolution cannot distinctively extract the spatial and spectral information around the central pixel, which makes the classiﬁcation results at the boundaries of ground objects over-smoothed and the classiﬁcation performance degraded. Thus, we propose a novel superpixel guided deformable convolution network (SGDCN) for hyperspectral image classiﬁcation. Firstly, the superpixel region fusion ﬁlter (SRF-Filter) is designed to fuse the initial superpixel region segmented by the simple linear iterative clustering (SLIC), making the fused superpixel region have a high homogeneity and also contain spatial features of diverse scales. Then, the superpixel guided deformable convolution (SGD-Conv) is proposed to make the shape of deformable convolution consistent with the real shape of land covers, and the SGD-Conv can extract pure neighborhood spatial-spectral features. Finally, a superpixel joint bilateral ﬁlter (SPJBF) is designed to solve the pixel-level and regionlevel misclassiﬁcation problem, which can effectively utilize the superpixel region’s homogeneity and improve the classiﬁcation accuracy. Experiments on three HSI datasets indicate that the SGDCN can obtain better classiﬁcation performance when compared with other twelve state-of-the-art methods.},
	language = {en},
	urldate = {2023-10-13},
	journal = {IEEE Transactions on Image Processing},
	author = {Zhao, Chunhui and Zhu, Wenxiang and Feng, Shou},
	year = {2022},
	pages = {3838--3851},
}

@article{hossain_segmentation_2019,
	title = {Segmentation for {Object}-{Based} {Image} {Analysis} ({OBIA}): {A} review of algorithms and challenges from remote sensing perspective},
	volume = {150},
	issn = {0924-2716},
	shorttitle = {Segmentation for {Object}-{Based} {Image} {Analysis} ({OBIA})},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271619300425},
	doi = {10.1016/j.isprsjprs.2019.02.009},
	abstract = {Image segmentation is a critical and important step in (GEographic) Object-Based Image Analysis (GEOBIA or OBIA). The final feature extraction and classification in OBIA is highly dependent on the quality of image segmentation. Segmentation has been used in remote sensing image processing since the advent of the Landsat-1 satellite. However, after the launch of the high-resolution IKONOS satellite in 1999, the paradigm of image analysis moved from pixel-based to object-based. As a result, the purpose of segmentation has been changed from helping pixel labeling to object identification. Although several articles have reviewed segmentation algorithms, it is unclear if some segmentation algorithms are generally more suited for (GE)OBIA than others. This article has conducted an extensive state-of-the-art survey on OBIA techniques, discussed different segmentation techniques and their applicability to OBIA. Conceptual details of those techniques are explained along with the strengths and weaknesses. The available tools and software packages for segmentation are also summarized. The key challenge in image segmentation is to select optimal parameters and algorithms that can general image objects matching with the meaningful geographic objects. Recent research indicates an apparent movement towards the improvement of segmentation algorithms, aiming at more accurate, automated, and computationally efficient techniques.},
	language = {en},
	urldate = {2023-04-18},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Hossain, Mohammad D. and Chen, Dongmei},
	month = apr,
	year = {2019},
	keywords = {Geographic object, High spatial resolution, Image segmentation, OBIA, Remote sensing},
	pages = {115--134},
}

@article{felzenszwalb_efficient_2004,
	title = {Efficient {Graph}-{Based} {Image} {Segmentation}},
	volume = {59},
	issn = {0920-5691},
	url = {http://link.springer.com/10.1023/B:VISI.0000022288.19776.77},
	doi = {10.1023/B:VISI.0000022288.19776.77},
	abstract = {This paper addresses the problem of segmenting an image into regions. We deﬁne a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an eﬃcient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two diﬀerent kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.},
	language = {en},
	number = {2},
	urldate = {2023-08-23},
	journal = {International Journal of Computer Vision},
	author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
	month = sep,
	year = {2004},
	pages = {167--181},
}

@article{achanta_slic_2012-1,
	title = {{SLIC} {Superpixels} {Compared} to {State}-of-the-{Art} {Superpixel} {Methods}},
	volume = {34},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2012.120},
	abstract = {Computer vision applications have come to rely increasingly on superpixels in recent years, but it is not always clear what constitutes a good superpixel algorithm. In an effort to understand the benefits and drawbacks of existing methods, we empirically compare five state-of-the-art superpixel algorithms for their ability to adhere to image boundaries, speed, memory efficiency, and their impact on segmentation performance. We then introduce a new superpixel algorithm, simple linear iterative clustering (SLIC), which adapts a k-means clustering approach to efficiently generate superpixels. Despite its simplicity, SLIC adheres to boundaries as well as or better than previous methods. At the same time, it is faster and more memory efficient, improves segmentation performance, and is straightforward to extend to supervoxel generation.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and Süsstrunk, Sabine},
	month = nov,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Approximation algorithms, Clustering algorithms, Complexity theory, Image color analysis, Image edge detection, Image segmentation, Measurement uncertainty, Superpixels, clustering, k-means, segmentation},
	pages = {2274--2282},
}

@inproceedings{audebert_how_2016,
	title = {How useful is region-based classification of remote sensing images in a deep learning framework?},
	doi = {10.1109/IGARSS.2016.7730327},
	abstract = {In this paper, we investigate the impact of segmentation algorithms as a preprocessing step for classification of remote sensing images in a deep learning framework. Especially, we address the issue of segmenting the image into regions to be classified using pre-trained deep neural networks as feature extractors for an SVM-based classifier. An efficient segmentation as a preprocessing step helps learning by adding a spatially-coherent structure to the data. Therefore, we compare algorithms producing superpixels with more traditional remote sensing segmentation algorithms and measure the variation in terms of classification accuracy. We establish that superpixel algorithms allow for a better classification accuracy as a homogenous and compact segmentation favors better generalization of the training samples.},
	booktitle = {2016 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium} ({IGARSS})},
	author = {Audebert, Nicolas and Le Saux, Bertrand and Lefèvre, Sébastien},
	month = jul,
	year = {2016},
	note = {ISSN: 2153-7003},
	keywords = {Deep learning, Feature extraction, Image classification, Image segmentation, Machine learning, Remote sensing, Segmentation algorithms, Semantics, Shape, Superpixels, Training},
	pages = {5091--5094},
}
